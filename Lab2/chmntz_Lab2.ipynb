{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOONexN3FEtbvB8IWh+NnJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT24/blob/chmntz_Lab2/Lab2/chmntz_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 2: Iterative methods**\n",
        "Carl **Chemntiz**"
      ],
      "metadata": {
        "id": "IHUigEoqbCO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**\n",
        "It this lab different types of iterative methods was implemented and discussed. Stationary iterative methods such as Jacobi iteration and Gauss-Seidel iteration was implemented to solve diagonally dominant system matrices and their benefits over direct methods was discussed. A Krylov subspace method, GMRES, that is able to solve a general square matrix was also implemented and its benefits over stationary iterative methods were presented. Lastly, the lab also contains a method that can solve nonlinear equations, scalar functions, Newton's method."
      ],
      "metadata": {
        "id": "TKrFUBWzbJLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set up environment**\n",
        "To have access to the necessary modules you have to run this cell."
      ],
      "metadata": {
        "id": "h1n2iC2EbMPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm, inv\n",
        "from scipy.optimize import fsolve\n",
        "\n",
        "# Iterative methods requires an arbitrary tolerance\n",
        "TOL = 1e-12"
      ],
      "metadata": {
        "id": "onORPYa9bQl-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "For massive systems of linear equations direct methods and matrix factorization will come at a huge computational cost, especially for dense matrices. For such systems, iterative methods are more memory efficient and at a lower computational cost by iteratively converging on an approximate solution $x$.\n",
        "\n",
        "There are two types of iterative methods, **stationary iterative methods** and **Krylov subspace methods**.  \n",
        "\n",
        "## Stationary iterative methods\n",
        "Stationary iterative methods are formulated as a linear fixed point iteration of the form\n",
        "$$x^{(k+1)}=Mx^{(k)}+c,$$\n",
        "where $M\\in\\mathbb{R}^{n\\times n}$ is the iteration matrix and $c\\in\\mathbb{R}^n$ is the constant vector. The converge criterion is that $\\|M\\|< 1$.\n",
        "\n",
        "The most simple stationary iterative method is Richardson iteration with $M_R=I-\\alpha A$ and $c=\\alpha b$,\n",
        "$$x^{(k+1)}=(I-\\alpha A)x^{(k)}+\\alpha b,$$\n",
        "where $\\alpha$ is the step length. Convergance can be improved by precondition the system by multiplying both sides by an approximate inverse of $A$, $B\\approx A^{-1}$ from the left,\n",
        "$$BAx=Bb,$$\n",
        "where $x$ still is the same solution. This precondition is left preconditioning, which for Richardson iteration gives left preconditioned Richardson iteration,\n",
        "$$x^{(k+1)}=(I-\\alpha BA)x^{(k)}+\\alpha Bb.$$\n",
        "There also exists right preconditioning, however, it is not relevant to this lab.\n",
        "\n",
        "Matrix splitting an technique to formulate an system of linear equations, $Ax=b$, as a stationary iterative methods but splitting the system matrix into a sum,\n",
        "$$A=A_1+A_2,$$\n",
        "where $A_1$ is a nonsingulat matrix which is easy to invert.\n",
        "\n",
        "## Krylov subspace methods\n",
        "Krylov subspace method approximates an solution to the system of linear equations $Ax=b$,\n",
        "$x^{(k)}\\approx x$\n",
        "in the Krylov subspace of $\\mathbb{R}^n$.\n",
        "The Krylov subspace is the subspace consturcted by $A$ and $b$,\n",
        "$$\\kappa_k=\\text{span}(b,Ab,\\dots,A^{k-1}b).$$\n",
        "\n",
        "## Newtons method\n",
        "There also exists iterative methods to approximate solutions (roots) to nonlinear equations. One such method is Newton's method which converges on a solution from an initial guess. However, it requires that the residual is differentiable."
      ],
      "metadata": {
        "id": "21TkdmaBbXKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Methods**"
      ],
      "metadata": {
        "id": "vhy7OrPWbh2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi iteration for $Ax=b$\n",
        "Jacobi iteration is an algorithm to approximate solution $x$ to $Ax=b$ where $A$ is a strictly diagonally dominant matrix.\n",
        "$$|a_{ii}|>\\sum_{j\\neq u}|a_{ij}|.$$\n",
        "The method is based on matrix splitting using a diagonal matrix,\n",
        "$$A_1=D,\\hspace{1em}A_2=A-D,$$\n",
        "where $D=\\text{diag}(A)$.\n",
        "This gives the iteration matrix $M_J$\n",
        "$$M_J=-L^{-1}(A-L)=I-D^{-1}Am$$\n",
        "and the constant vector $c$\n",
        "$$c=D^{-1}b.$$\n",
        "Since matrix splitting is a stationary iterative method, we get that the linear iteration\n",
        "\n",
        "$$x^{(k+1)}=M_Jx^{(k)}+c=(I-D^{-1}A)x^{(k)}+D^{-1}b.$$\n",
        "Jacobi iteration is equivalent to left Jacobi preconditioned Richardson iteration with $B=D^{-1}$ and $\\alpha=1$.\n",
        "\n",
        "From chapter 7, example 7.8 presents the Jacobi iteration in terms of the components of $A=(a_{ij})$,\n",
        "\n",
        "$$x_i^{(k+1)}=a_{ii}^{-1}\\Big(b_i-\\sum_{j\\neq i}a_{ij}x_j^{(k)}\\Big),\\hspace{1em}\\forall i$$"
      ],
      "metadata": {
        "id": "XiSkH3Hrtrc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jacobi_iteration(A: np.array, b: np.array) -> np.array:\n",
        "    n = len(b)\n",
        "    x = np.zeros(n)\n",
        "\n",
        "    while norm(A @ x - b) > TOL:\n",
        "        x_k = x.copy()\n",
        "        for i in range(n):\n",
        "            a_inv = 1 / A[i,i]\n",
        "            row_sum = 0\n",
        "            for j in range(n):\n",
        "                if j == i: continue\n",
        "                row_sum += A[i,j] * x_k[j]\n",
        "            x[i] = a_inv * (b[i] - row_sum)\n",
        "    return x"
      ],
      "metadata": {
        "id": "O3P7_9vGtq_F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, convergance of an iterative method can be improved by using *relaxation*. Relaxation is an method of setting the degree of diagonal dominance by a parameter $\\omega>0$. This gives us the damped Jacobi iteration.\n",
        "$$x^{(k+1)}=x^{(k)}+\\omega D^{-1}(b-Ax^{(k)})$$\n",
        "The book (chapter 7, section 7) also suggests an technique to improve convergance even further using *successive over-relaxation*, however, this technique was not implemented."
      ],
      "metadata": {
        "id": "t-0QTNBESWl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def damped_jacobi_iteration(A: np.array, b: np.array) -> np.array:\n",
        "    x = b.copy()\n",
        "    D = np.diag(np.diag(A))\n",
        "    M = np.eye(len(b)) - inv(D) @ A\n",
        "    # c = inv(D) @ b\n",
        "    w = 0.7\n",
        "\n",
        "    while norm(A @ x - b) > TOL:\n",
        "        # x = M @ x + c\n",
        "        x = x + w * inv(D) @ (b - A @ x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "x7aYMnjnse8l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gauss-Seidel iteration for $Ax=b$\n",
        "Gauss-Seidel iteration is similar to Jacobi iteration, using the same principles but splitting the matrix using the lower triangular matrix,\n",
        "$$A_1=L,\\hspace{1em}A_2=A-L.$$\n",
        "Unlike Jacobi iteration, $A$ does not have to be strictly diagonally dominant, and can be irreducibly diagonally dominant aswell.\n",
        "Similarily the iteration matrix is\n",
        "$$M_{GS}=-L^{-1}(A-L)=I-L^{-1}A,$$\n",
        "and the vector becomes\n",
        "$$c=L^{-1}b.$$\n",
        "Gauss-Seidel iteration is equivalent to left preconditioned Richardson iteration with $B=L^{-1}$ and $\\alpha=1$.\n",
        "\n",
        "The form expressed in terms of components is\n",
        "\n",
        "$$x_i^{(k+1)}=a_{ii}^{-1}\\Big(b_i-\\sum_{j<i}a_{ij}x_j^{(k+1)}-\\sum_{j>i}a_{ij}x_j^{(k)}\\Big),\\hspace{1em}\\forall i$$"
      ],
      "metadata": {
        "id": "_gkNyx-7t4vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gauss_seidel_iteration(A: np.array, b: np.array) -> np.array:\n",
        "    n = len(b)\n",
        "    x = np.zeros(n)\n",
        "\n",
        "    while norm(A @ x - b) > TOL:\n",
        "        x_k = x.copy()\n",
        "        for i in range(n):\n",
        "            a_inv = 1 / A[i,i]\n",
        "            row_sum = 0\n",
        "            for j in range(i):\n",
        "                row_sum += A[i,j] * x[j]\n",
        "            for j in range(i+1, n):\n",
        "                row_sum += A[i,j] * x_k[j]\n",
        "            x[i] = a_inv * (b[i] - row_sum)\n",
        "    return x"
      ],
      "metadata": {
        "id": "loNQXb6-u1ae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Gauss-Seidel iteration is a stationary iterative method, the method can be damped using relaxation."
      ],
      "metadata": {
        "id": "NLxLkMIfxvxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def damped_gauss_seidel_iteration(A: np.array, b: np.array) -> np.array:\n",
        "    x = b.copy()\n",
        "    L = np.tril(A)\n",
        "    M = np.eye(len(b)) - inv(L) @ A\n",
        "    c = inv(L) @ b\n",
        "    w = 0.7\n",
        "\n",
        "    while norm(A @ x - b) > TOL:\n",
        "        # x = M @ x + c\n",
        "        x = x + w * inv(L) @ (b - A @ x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XUpIehQhuEHz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method for scalar nonlinear equation $f(x)=0$\n",
        "Newton's method is an iterative algorithm to approxiamte the roots of a function based on an initial guess. The function $f$ must be a differentiable function.\n",
        "$$x^{(k+1)}=x^{(k)}-\\frac{f(x^{(k)})}{f'(x^{(k)})}$$\n",
        "\n",
        "Since Newton's method requires the derivative of a function, a method to approximate the finite difference was implemented. Central difference was chosen due to its higher accuracy compared to forward and backward differences."
      ],
      "metadata": {
        "id": "rtN9sA97t80H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivate(f, x: float) -> float:\n",
        "    h = 1e-15\n",
        "    return (f(x + h) - f(x - h)) / (2 * h)"
      ],
      "metadata": {
        "id": "-yTSj5D7eA8E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab `newtons_method` was implemented based on algorithm 8.2 from the book *Methods in Computational Science*, which itself was an implementation of the equation stated above."
      ],
      "metadata": {
        "id": "dhQGJNREkG0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newtons_method(f, x: float) -> float:\n",
        "    while abs(f(x)) > TOL:\n",
        "        df = derivate(f, x)\n",
        "        if df == 0: raise ArithmeticError\n",
        "        x = x - (f(x) / df)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VGVNRF3iuD9v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMRES method for $Ax=b$\n",
        "Generalized minimal residual method (GMRES) approximates solution $x$ to the system of linear solution $Ax=b$ by minimizing the norm of the residual $r^{(k)}=b-Ax^{(k)}$.\n",
        "This can be done using the least squares problem\n",
        "$$\\min_{x^{(k)}\\in\\kappa_k}\\|b-Ax^{(k)}\\|.$$\n",
        "GMRES utilizes the Arnoldi algorithm which computes an orthonormal basis of the same space\n",
        "and thus the approximation can be expressed as $x^{(k)}=Q_\\kappa y$ with a vector $y\\in\\mathbb{R}^k$ with the coordinates of $x^{(k)}$ in the orthonormal basis. Thus,\n",
        "$$\\min_{y\\in\\mathbb{R}^k}\\|b-AQ_\\kappa y\\|.$$\n",
        "This makes the algorithm numerically stable. Had the approximation been expressed a linear combination of the Krylov vectors, the algorithm would have been numerically unstable.\n",
        "\n",
        "Arnoldi iteration is an algorithm to approximate eigenvalues and eigenvectors of a $n\\times n$ matrix $A$ by constructing an orthonormal basis $Q_k$,\n",
        "$$AQ_k=Q_{k+1}H_k,$$\n",
        "where $Q$ is the basis vectors of the Krylov subspace and $H$ is the upper Hessenberg matrix. It is based on modified Gram-Schmidt iteration with an initial vector $b$ which is used to get the first element of $Q$, $q_1=b/\\|b\\|$.\n",
        "\n",
        "Arnoldi iteration was implemented based on algorithm 7.3 from the book *Methods in Computation Science*."
      ],
      "metadata": {
        "id": "KkVOZS2TwcSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arnoldi_iteration(A: np.array, b: np.array, k: int) -> np.array:\n",
        "    Q = np.zeros((A.shape[0], k+1))\n",
        "    H = np.zeros((k+1, k))\n",
        "    Q[:,0] = b / norm(b)\n",
        "\n",
        "    for j in range(1,k+1):\n",
        "        v_j = A @ Q[:,j-1]\n",
        "        for i in range(0,j):\n",
        "            H[i,j-1] = Q[:,i] @ v_j\n",
        "            v_j -= H[i,j-1] * Q[:,i]\n",
        "        H[j, j-1] = norm(v_j)\n",
        "        Q[:, j] = v_j / H[j, j-1]\n",
        "    return Q, H"
      ],
      "metadata": {
        "id": "TSYgFAGMGNaR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GMRES algorithm itself was implemented based on algorithm 7.2 from the book *Methods in Computational Science*. In this implementation, Numpy's `linalg.lstsq` function was used to solve least squares problem."
      ],
      "metadata": {
        "id": "WVM02fmNkdCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gmres(A: np.array, b: np.array) -> np.array:\n",
        "    k_max = 1000\n",
        "    Q = np.zeros((A.shape[0], k_max))\n",
        "    Q[:,0] = b / norm(b)\n",
        "\n",
        "    for k in range(1, k_max):\n",
        "        e1 = np.zeros(k+1)\n",
        "        e1[0] = 1\n",
        "\n",
        "        Q, H = arnoldi_iteration(A, b, k)\n",
        "        y = np.linalg.lstsq(H, norm(b)*e1, rcond=None)[0]\n",
        "        r = (norm(b) * e1) - (H @ y)\n",
        "\n",
        "        if norm(r) < TOL: break;\n",
        "    x = Q[:, 0:k] @ y\n",
        "    return x"
      ],
      "metadata": {
        "id": "XxhsM2N4KGkt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "8V3Sz61Kbm1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi iteration for $Ax=b$\n",
        "A psuedorandom strictly diagonally dominant system matrix $A$ and psuedorandom vector $b$ of the same length is constructed."
      ],
      "metadata": {
        "id": "T_o_duXhM--j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.random((5,5)) + 5 * np.eye(5)\n",
        "b = np.random.rand(5)"
      ],
      "metadata": {
        "id": "joKnwUcymJXA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convergance of residual test $||Ax-b||$ is done by calculating $Ax$, where $x$ is the approximated solution and comparing it to vector $b$. If they are the same, i.e. $||Ax-b||=0$, then the algorithm converged on the true solution.\n",
        "\n",
        "As two algorithm were implemented, the test was done on both."
      ],
      "metadata": {
        "id": "PB65io7ON8np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||Ax-b||=0: {np.allclose(A @ jacobi_iteration(A,b), b)}\")\n",
        "print(f\"||Ax-b||=0: {np.allclose(A @ damped_jacobi_iteration(A,b), b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX1jNnBmM-de",
        "outputId": "8dfcd1b4-65f3-45c1-be79-e0a557eb6459"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax-b||=0: True\n",
            "||Ax-b||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Numpy's own solver of system of linear equations to calculate the solution, it was compared to the user implemented solution. If they are the same, the user implemented solution converged on the same solution as the pre-built algorithm."
      ],
      "metadata": {
        "id": "5stHHuF3OLxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||x-y||=0: {np.allclose(np.linalg.solve(A,b), jacobi_iteration(A,b))}\")\n",
        "print(f\"||x-y||=0: {np.allclose(np.linalg.solve(A,b), damped_jacobi_iteration(A,b))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIJmPJyANO9B",
        "outputId": "c667372b-3214-4cc4-94ad-dddf36e4cdcc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||x-y||=0: True\n",
            "||x-y||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gauss-Seidel iteration for $Ax=b$\n",
        "A psuedorandom strictly diagonally dominant system matrix $A$ and psuedorandom vector $b$ of the same length is constructed."
      ],
      "metadata": {
        "id": "nBF207Eemyz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.random((5,5)) + 5 * np.eye(5)\n",
        "b = np.random.rand(5)"
      ],
      "metadata": {
        "id": "wUiuORJ8nXx9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convergance of residual test $||Ax-b||$ is done by calculating $Ax$, where $x$ is the approximated solution and comparing it to vector $b$. If they are the same, i.e. $||Ax-b||=0$, then the algorithm converged on the true solution."
      ],
      "metadata": {
        "id": "EkSrnORin36S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||Ax-b||=0: {np.allclose(A @ gauss_seidel_iteration(A,b), b)}\")\n",
        "print(f\"||Ax-b||=0: {np.allclose(A @ damped_gauss_seidel_iteration(A,b), b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0xfQfqbnGls",
        "outputId": "aa951a7c-57d0-4d6f-c526-c612463c2ff9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax-b||=0: True\n",
            "||Ax-b||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Numpy's own solver of system of linear equations to calculate the solution, it was compared to the user implemented solution. If they are the same, the user implemented solution converged on the same solution as the pre-built algorithm."
      ],
      "metadata": {
        "id": "7va7erv6n5V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||x-y||=0: {np.allclose(np.linalg.solve(A,b), gauss_seidel_iteration(A,b))}\")\n",
        "print(f\"||x-y||=0: {np.allclose(np.linalg.solve(A,b), damped_gauss_seidel_iteration(A,b))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COgzQNFhylwh",
        "outputId": "382edd3e-a61e-45f8-d436-bf3770c916a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||x-y||=0: True\n",
            "||x-y||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method for scalar nonlinear equation $f(x)=0$\n",
        "A nonlinear equation is declared as $f(x)$ and newton's method is used to approximate the root with an initial guess of $x_0=0$."
      ],
      "metadata": {
        "id": "iei-WiUia4Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: 4 * x**3 + 2 * x**2 + 8 * x + 6\n",
        "x = newtons_method(f, 0)"
      ],
      "metadata": {
        "id": "s0bh-9WWa_WB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The approximated solution $x$ is inserted into the function $f$ and should yeild 0 if the algorithm was implemented correctly."
      ],
      "metadata": {
        "id": "lHArPu9Csc-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"|f(x)|=0: {np.isclose(f(x),0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5pwjhytcPUd",
        "outputId": "f6b79d45-98c5-43df-e3fb-dc3440b33ee0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|f(x)|=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The approximated solution $x$ was compared to solution calculated by Scipy's `fsolve` function. If the algorithm was correctly implemented, they should be the same."
      ],
      "metadata": {
        "id": "ygyPPn8isrvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"|x-y|=0: {np.isclose(x, fsolve(f, 0))[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuGOvgfuc2Ne",
        "outputId": "65d6e7ce-2e68-40fb-8882-29c883e44e84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|x-y|=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMRES methods for $Ax=b$\n",
        "A psuedorandom system of linear equations, $Ax=b$, was constructed with an unknown solution $x$. Since GMRES is an Krylov subspace method, it should be able to solve any general $n\\times n$ matrix."
      ],
      "metadata": {
        "id": "0Q7uS7_vaiRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.random((5,5))\n",
        "b = np.random.rand(5)"
      ],
      "metadata": {
        "id": "bwgWjsOUl4j_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convergance of residual test $||Ax-b||$ is done by calculating $Ax$, where $x$ is the approximated solution and comparing it to vector $b$. If they are the same, i.e. $||Ax-b||=0$, then the algorithm converged on the true solution."
      ],
      "metadata": {
        "id": "VVUsBwzWoY_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||Ax-b||=0: {np.allclose(A @ gmres(A,b), b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrvtBFXyaiDF",
        "outputId": "62e89cc8-316a-4ab9-c17a-e76acae302cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax-b||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Numpy's own solver of system of linear equations to calculate the solution, it was compared to the user implemented solution. If they are the same, the user implemented solution converged on the same solution as the pre-built algorithm."
      ],
      "metadata": {
        "id": "-N8pW87koacI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"||x-y||=0: {np.allclose(np.linalg.solve(A,b), gmres(A,b))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROdaGCRNawiq",
        "outputId": "b3701d89-2320-411a-ff63-de2447c6d991"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||x-y||=0: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion**\n",
        "All results were as expected and all implmented algorithms passed their tests.\n",
        "It was particular interesting to me that both Jacobi and Gauss-Seidel iteration can ee implemented using matrices and in terms of components. It should be noted that damped Jacobi and Gauss-Seidel iteration can be implemented in terms of components aswell, but due to time restrictions I only implemented the damped version using matrices. It would also have been interesting to measure which of the implementations (matrices vs component) was faster. In theory, the matrix implementation should be faster as linear algebra is very optimized on modern software.\n",
        "\n",
        "While the stationary iterative methods were more simple to implement, GMRES was more useful, able to compute any general square matrix. I chose to use the already implemented least squares problem algorithm in this lab, however, I could also have used the implementation I did during the Matrix factorization lab. Since there was no restrictions of using already existing functions, there was no reason to use the user implemented solution."
      ],
      "metadata": {
        "id": "FYNGb60Kbom_"
      }
    }
  ]
}