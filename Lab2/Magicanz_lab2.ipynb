{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Iterative Methods**\n",
        "**Teo Nordström**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "This file contains the solutions to the three mandatory problems from Lab2 in DD2363, in addition to the solution to one of the optional problems. It is based upon pseudocode and info found in *Methods in Computational Science* by Johan Hoffman (2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dff1e8a2-bcc0-4dd5-f850-8c2071802d47"
      },
      "source": [
        "\"\"\"This file is based on a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# TEMPLATE INFO:\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se\n",
        "\n",
        "# CODE INFO:\n",
        "# Code written by Teo Nordström 2024, no license."
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "These are the neccessary modules for everything in this file to work.\n",
        "\n",
        "Also set up some functions from a previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def modified_gram_schmidt_iteration(A:np.ndarray):\n",
        "    n = len(A)\n",
        "    Q = np.zeros((n, n))\n",
        "    R = np.zeros((n, n))\n",
        "    for j in range(n):\n",
        "        v = A[:, j]\n",
        "        for i in range(j):\n",
        "            R[i, j] = np.dot(Q[:, i], v)\n",
        "            v = v - R[i, j] * Q[:, i]\n",
        "        R[j, j] = np.linalg.norm(v)\n",
        "        Q[:, j] = v / R[j, j]\n",
        "    return Q, R\n",
        "\n",
        "\n",
        "def backward_substitution(U:np.ndarray, b:np.ndarray):\n",
        "    n = U.shape[1]\n",
        "    x = np.zeros(n)\n",
        "    x[n-1] = b[n-1] / U[n-1, n-1]\n",
        "    for i in range(n-2, -1, -1):\n",
        "        sum = 0\n",
        "        for j in range(i+1, n):\n",
        "            sum += U[i, j] * x[j]\n",
        "        x[i] = (b[i] - sum) / U[i, i]\n",
        "    return x\n",
        "\n",
        "\n",
        "def direct_solver(A:np.ndarray, b:np.ndarray):\n",
        "    Q, R = modified_gram_schmidt_iteration(A)\n",
        "    y = np.transpose(Q).dot(b)\n",
        "    x = backward_substitution(R, y)\n",
        "    return x\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "All solutions will be partially or entirely based upon the book *Methods in Computational Science* by Johan Hoffman (2021). In the text, it will be referred to as the \"course book\".\n",
        "\n",
        "# Jacobi Iteration\n",
        "Jacobi Iteration is a version of Richardson Iteration that uses matrix splitting and precondition to find $x$ in a problem $Ax = n$. It is an iterative solution meaning it for each iteration should get closer to the answer, as long as $||I - D^{-1}A|| < 1||$ (where $D$ is the diagonal of $A$), since otherwise it will not converge. It it is more efficient than normal Richardson Iteration when matrix A is diagonally dominant.\n",
        "\n",
        "# Gauss-Seidel Iteration\n",
        "Gauss-Seidel Iteration is another version of Richardson Iteration that uses matrix splitting and precondition. Here, instead of using the diagonal $D$ we use the lower triangular matrix $L$ to split up $A$. $L$ is just $A$ with all non-lower-triangular values zeroed out. The convergence criterion here is $||I - L^{-1}A|| < 1||$ which once again means it works on diagonally dominant matrices.\n",
        "\n",
        "# Newton's Method for Scalar Nonlinear Equation\n",
        "Newton's Method is an iterative method to find a root of a function. For a function $f$, this would be to look for a value $x$ where $f(x) = 0$. It is a fixed point iteration that has a quadratic order of convergence. The fixed point iteration is on the form $x^{(k+1)}=x^{(k)} + \\alpha f(x^{(k)})$ where for it to be Newton's Method $a = -f'(x^{(k)})^{-1}$. This would essentially mean that you are to divide $x^{(k)}$ by the derivative of itself to get the second term in the iteration.\n",
        "\n",
        "# Newton's Method for Vector Nonlinear Equation\n",
        "Newton's Method for Vectors is an iterative method to find solutions for $f(x) = 0$ in a nonsingular system of linear equations. There is no guarantee to find an answer as there is for linear systems using this method, but it can still be used in the cases that there exists one (or can be used to figure out IF there exists one. Similar to scalars, the fixed point iteration is on the form $x^{(k+1)}=x^{(k)} +Af(x^{(k)})$ where $A = -f'(x^{(k)})^{-1}$. $f'(x)$ for a system of equations is stated as a Jacobian, which is essentially all the different derivatives for the different variables in each separate equation in the vector function. The Jacobian matrix is on the form\n",
        "\n",
        "$f'(x) = J = \\begin{bmatrix}\n",
        "\\frac{df_1(x)}{dx_1(x)} & ... & \\frac{df_1(x)}{dx_n(x)}\\\\\n",
        "... & ... & ...\\\\\n",
        "\\frac{df_n(x)}{dx_1(x)} & ... & \\frac{df_n(x)}{dx_n(x)}\\\\\n",
        "\\end{bmatrix}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "# Jacobi Iteration\n",
        "To figure out an implementation of Jacobi Iteration we use both information about Richardson Iteration and Jacobi Iteration. The method that was decided to be used is loosely based upon the pseudocode from Algorithm 7.1 in the course book, but with the component form $x_i^{(k+1)} = a_{ii}^{-1} (b_i - \\sum_{j\\neq i} a_{ij} x_j^{(k)}) \\forall i$ used in the main loop of the iteration taken from examle 7.8 in the course book."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jacobi_iteration(A:np.ndarray, b:np.ndarray):\n",
        "    n = b.size\n",
        "    x = np.zeros(n)\n",
        "    while np.linalg.norm(A @ x - b) > 10**-15:\n",
        "        for i in range(n):\n",
        "            total = 0\n",
        "            for j in range(n):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                total += A[i, j] * x[j]\n",
        "            x[i] = (1/A[i, i]) * (b[i] - total)\n",
        "    return x\n",
        "\n",
        "\n",
        "ji_A = np.array([[2, -1], [-1, 2]])\n",
        "ji_b = np.array([1, 2])\n",
        "\n",
        "print(jacobi_iteration(ji_A, ji_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_EGvvispQuP",
        "outputId": "f5977ffa-26cb-478d-9c66-7be99264489c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.33333333 1.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gauss-Seidel Iteration\n",
        "\n",
        "The Gauss-Seidel Iteration implementation is very similar to the Jacobi Iteration implementation, just with using a different splitting and a different component form. This time, $x_i^{(k+1)} = a_{ii}^{-1} (b_i - \\sum_{j< i} a_{ij} x_j^{(k+1)} - \\sum_{j> i} a_{ij} x_j^{(k+1)}) \\forall i$, which is an equation taken from Example 7.9 in the course book. Another difference is the splitting being based upon a lower triangular matrix meaning we need a method to invert it. This is done using forward substitution trying to figure out the solution to $Lx_i = I_{:,i}$ for all $0 ... n$ where $x_i$ is the $i$:th column in the inverted matrix. The forward substitution is based upon pseudocode from Algorithm 5.1 in the course book."
      ],
      "metadata": {
        "id": "egnKCwCGvQj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_substitution_inverter(L:np.ndarray):\n",
        "    n = L.shape[0]\n",
        "    identity = np.identity(n)\n",
        "    x = np.zeros(L.shape)\n",
        "    for c in range(n):\n",
        "        b = identity[:, c]\n",
        "        x[0, c] = b[0] / L[0, 0]\n",
        "        for i in range(n):\n",
        "            sum = 0\n",
        "            for j in range(i):\n",
        "                sum += L[i, j] * x[j, c]\n",
        "            x[i, c] = (b[i] - sum) / L[i, i]\n",
        "    return x\n",
        "\n",
        "\n",
        "def gauss_seidel(A:np.ndarray, b:np.ndarray):\n",
        "    n = b.size\n",
        "    x = np.zeros(n)\n",
        "    atri = np.tril(A)\n",
        "    atriinv = forward_substitution_inverter(atri)\n",
        "    while np.linalg.norm(A @ x - b) > 10**-15:\n",
        "        for i in range(n):\n",
        "            total = 0\n",
        "            for j in range(n):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                total += A[i, j] * x[j]\n",
        "            x[i] = (atriinv[i, i]) * (b[i] - total)\n",
        "    return x\n",
        "\n",
        "\n",
        "gs_A = np.array([[2, -1], [-1, 2]])\n",
        "gs_b = np.array([1, 2])\n",
        "\n",
        "print(gauss_seidel(gs_A, gs_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pAJtD9Kv4bn",
        "outputId": "c5f52e9a-4173-4387-d5e3-c0267c8daa6c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.33333333 1.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method for Scalar Nonlinear Equation\n",
        "\n",
        "This implementation of Newton's method is based upon the information in chapter 8.3 in the course book, and specifically based on the pseudocode in Algorithm 8.2. It performs the equation defined in the introduction on an arbitrary function, in this example $3x^3 - 2x^2 - x$ which has roots at $-\\frac{1}{3}, 0$ and $1$. To calculate the derivative I have implemented my own method based upon the central difference formula, which will try a bigger and bigger value of h until it finds a good derivative (the smaller the better but sometimes too small does not work)."
      ],
      "metadata": {
        "id": "M6FR-GNhzsPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def a_nonlinear_function(x):\n",
        "    return 3*x**3 - 2*x**2 - x  # The Function 3x^3 - 2x^2 - x which has roots at 0, 1 and -1/3\n",
        "\n",
        "\n",
        "def derivative(f, x):\n",
        "    h = 10**-15\n",
        "    for i in range(6):\n",
        "        df = (f(x + h) - f(x - h)) / (2 * h)\n",
        "        if df != 0:\n",
        "            return df\n",
        "        h *= 100\n",
        "    raise ValueError(\"Derivative is zero, x already found.\")\n",
        "\n",
        "\n",
        "def newtons_method(f, x0):\n",
        "    x = x0\n",
        "    func_val = f(x)\n",
        "    while abs(func_val) > 10**-15:\n",
        "        x = x - func_val / derivative(f, x)\n",
        "        func_val = f(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "print(newtons_method(a_nonlinear_function, 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIULzY2Y2C7O",
        "outputId": "3ef6465b-97d2-4787-f6ec-56c2adbcb745"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method for Vector Nonlinear Equation\n",
        "\n",
        "This implementation of Newton's Method for vectors is based upon the information in chapter 8.4 in the course book. Most notably, the main function is based on algorithm 8.4 from the same chapter. To solve the problem, we also had to bring in some help from the previous assignment, this being the direct solver that used QR Factorization to solve a problem $Ax = b$. We also had to create a function that calculates the derivatives in the jacobian matrix, which is done by using the forward difference formula on the specific value we are interested in."
      ],
      "metadata": {
        "id": "yP5CY3YJ2m4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def a_nonlinear_system(x:np.ndarray):\n",
        "    a = np.array([np.sin(x[1] + 2*x[0]), np.cos(x[0]) + 0.5])\n",
        "    return a\n",
        "\n",
        "\n",
        "def jacobian(f, x:np.ndarray):\n",
        "    h = 10**-8\n",
        "    n = x.size\n",
        "    J = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            f_old_x = f(x)[i]\n",
        "            x[j] += h\n",
        "            df = f(x)[i] - f_old_x\n",
        "            J[i, j] = df / h\n",
        "    return J\n",
        "\n",
        "\n",
        "def newtons_method_system(f, x0:np.ndarray):\n",
        "    x = x0\n",
        "    func_vector = f(x)\n",
        "    while np.linalg.norm(func_vector) > 10**-6:\n",
        "        Df = jacobian(f, x)\n",
        "        dx = direct_solver(Df, -func_vector)\n",
        "        x = x + dx\n",
        "        func_vector = f(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "print(newtons_method_system(a_nonlinear_system, np.array([1, 1.5])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FdxcPeL4QL-",
        "outputId": "d19f5d1c-ae76-4887-f7b0-3efdca12d278"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.09439512 -1.04719753]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "In this section tests will be performed to verify that the solutions are correct\n",
        "\n",
        "# Jacobi Iteration\n",
        "To test the Jacobi Iteration we start by generating a random square matrix of size $n \\times n$. Since all the values are between $[0, 1]$ we also add the identity matrix to it to make sure that it is diagonally dominant. We then generate a vector that is our $b$. We now check two residuals to see whether we achieve the correct answer. First, we test the residual $||Ax - b||$. We also manufacture our own solution $b = Ay$, which we test in the residual $||x_y - y||$ to see whether the real solution matches the generated one. These should both approach zero, and for this code they do."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ji_test(iters):\n",
        "    for test in range(iters):\n",
        "        n = np.random.randint(2, 10)\n",
        "        matrix = np.random.rand(n, n) + np.identity(n)\n",
        "        vector = np.random.rand(n)\n",
        "\n",
        "        x = jacobi_iteration(matrix, vector)\n",
        "        res_axb = np.linalg.norm(matrix @ x - vector)\n",
        "\n",
        "        y = np.random.rand(n)\n",
        "        x_y = jacobi_iteration(matrix, matrix @ y)\n",
        "        res_xy = np.linalg.norm(x_y - y)\n",
        "        print(f\"Test {test}: ||Ax-b|| = {res_axb}, ||x-y|| = {res_xy}\")\n",
        "\n",
        "\n",
        "ji_test(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5LMH60OBKsP",
        "outputId": "ab817c9e-003d-411e-d78b-890d911e34a3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: ||Ax-b|| = 1.1443916996305594e-16, ||x-y|| = 2.2887833992611187e-16\n",
            "Test 1: ||Ax-b|| = 9.42898409468251e-16, ||x-y|| = 9.41467529262489e-16\n",
            "Test 2: ||Ax-b|| = 5.551115123125783e-17, ||x-y|| = 3.1401849173675503e-16\n",
            "Test 3: ||Ax-b|| = 7.043575467574348e-16, ||x-y|| = 3.79805414435608e-15\n",
            "Test 4: ||Ax-b|| = 3.5652682413747997e-16, ||x-y|| = 5.4672143489065705e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Gauss-Seidel Iteration\n",
        "\n",
        "To test the implementation of Gauss-Seidel Iteration, we use the exact same test cases as for Jacobi Iteration. Info can be found in the section above."
      ],
      "metadata": {
        "id": "JJc3ibYgBMAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gs_test(iters):\n",
        "    for test in range(iters):\n",
        "        n = np.random.randint(2, 10)\n",
        "        matrix = np.random.rand(n, n) + np.identity(n)\n",
        "        vector = np.random.rand(n)\n",
        "\n",
        "        x = gauss_seidel(matrix, vector)\n",
        "        res_axb = np.linalg.norm(matrix @ x - vector)\n",
        "\n",
        "        y = np.random.rand(n)\n",
        "        x_y = gauss_seidel(matrix, matrix @ y)\n",
        "        res_xy = np.linalg.norm(x_y - y)\n",
        "        print(f\"Test {test}: ||Ax-b|| = {res_axb}, ||x-y|| = {res_xy}\")\n",
        "\n",
        "\n",
        "gs_test(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGIKfpRhBMSA",
        "outputId": "96f1ab6c-7b60-4d34-91f3-a26c024b7190"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: ||Ax-b|| = 5.721958498152797e-16, ||x-y|| = 7.301351988545593e-16\n",
            "Test 1: ||Ax-b|| = 9.805224261780596e-16, ||x-y|| = 4.002966042486721e-16\n",
            "Test 2: ||Ax-b|| = 5.117875266520903e-16, ||x-y|| = 6.391696875268275e-16\n",
            "Test 3: ||Ax-b|| = 9.852251986074894e-16, ||x-y|| = 5.324442579404919e-16\n",
            "Test 4: ||Ax-b|| = 6.933340566559918e-16, ||x-y|| = 4.150409932595612e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Newton's Method for Scalar Nonlinear Equation\n",
        "\n",
        "To test that the Newton's method works we first select a root and then create a non-linear function that will have a singular root which is at the point we selected. This is done by creating a parabola at the form $(x - root)^2$. We then generate a random first try for x0 and run the method. The output will then be put into the parabola and compared to the created root to see if they align."
      ],
      "metadata": {
        "id": "hWIeJx6OBMhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nm_test(iters):\n",
        "    for test in range(iters):\n",
        "        root = np.random.rand() * 200 - 100\n",
        "\n",
        "        def f(x):\n",
        "            return (x - root)**2\n",
        "\n",
        "        x0 = np.random.rand() * 200 - 100\n",
        "        xs = newtons_method(f, x0)\n",
        "        res_f = abs(f(xs))\n",
        "        res_xy = abs(xs - root)\n",
        "        print(f\"Test {test}: Root = {root}, |f(x)| = {res_f}, |x-y| = {res_xy}\")\n",
        "\n",
        "\n",
        "nm_test(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hge11-v_BMww",
        "outputId": "9e136d8e-2758-4eea-d823-c472587a1906"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: Root = 2.566930617745385, |f(x)| = 7.0430861504069995e-16, |x-y| = 2.6538813369114678e-08\n",
            "Test 1: Root = -71.24788019813548, |f(x)| = 3.4171874323933246e-16, |x-y| = 1.8485636132936634e-08\n",
            "Test 2: Root = 92.77561525014494, |f(x)| = 3.482673247347898e-16, |x-y| = 1.8661921785678715e-08\n",
            "Test 3: Root = -84.2099627684415, |f(x)| = 7.862475079682518e-16, |x-y| = 2.8040105348736688e-08\n",
            "Test 4: Root = 95.30452190181441, |f(x)| = 7.096969104250101e-16, |x-y| = 2.6640137207323278e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's Method for Vector Nonlinear Equation\n",
        "\n",
        "To test that the Newton's method works we create a nonlinear system of equations with only one solution. The selected test system is a system with one circle of radius 1 ($(x-offset)^2 + y^2 - 1 = 0$) and one $x=y$ line with a single tangent. This is offset in the $x$ axis by a random variable."
      ],
      "metadata": {
        "id": "FSFS1_8FBQjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nms_test(iters):\n",
        "    for test in range(iters):\n",
        "        offset = np.random.rand() * 2 - 1\n",
        "        root = np.array([-math.cos(math.pi/4) + offset, math.cos(math.pi/4)])\n",
        "\n",
        "        def f(x: np.ndarray):\n",
        "            a = np.array([(x[0] - offset)**2 + (x[1])**2 - 1, x[0] - x[1] + math.sqrt(2) - offset])\n",
        "            return a\n",
        "\n",
        "        xs = newtons_method_system(f, np.array([np.random.rand(), np.random.rand()]))\n",
        "        res_f = np.linalg.norm(f(xs))\n",
        "        res_xy = np.linalg.norm(xs - root)\n",
        "        print(xs)\n",
        "        print(f\"Test {test}: Offset = {offset} ||f(x)|| = {res_f}, ||x-y|| = {res_xy}\")\n",
        "\n",
        "\n",
        "nms_test(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl64D32nBQzJ",
        "outputId": "4ce106b2-2745-4936-cfa0-5fcb71ce9284"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.72399225  0.70759804]\n",
            "Test 0: Offset = -0.017376727842931095 ||f(x)|| = 4.826735251484138e-07, ||x-y|| = 0.0006947470943838688\n",
            "[-0.84520346  0.70755102]\n",
            "Test 1: Offset = -0.13854092484979041 ||f(x)|| = 3.9470268498043026e-07, ||x-y|| = 0.0006282536787710146\n",
            "[-1.15480138  0.70774003]\n",
            "Test 2: Offset = -0.4483278499703831 ||f(x)|| = 8.02010061251579e-07, ||x-y|| = 0.0008955501441386554\n",
            "[-0.40546402  0.70758185]\n",
            "Test 3: Offset = 0.3011676884449985 ||f(x)|| = 4.5138933790767055e-07, ||x-y|| = 0.0006718551463542368\n",
            "[-0.13102981  0.70757875]\n",
            "Test 4: Offset = 0.5756050046388395 ||f(x)|| = 4.4552079140087056e-07, ||x-y|| = 0.0006674678876142447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The methods this week required a bit more experimentation than the ones created last week, though it made them feel more like one's own work instead of just a rewriting of some pseudocode. The Newton's Method for Vector Nonlinear Equation took much more work to implement than I had first expected, but once a way was found to create it it was incredibly satisfying.\n",
        "\n",
        "When it comes to the test, we can see that not everything is quite as good as it was last week. All tests are obviously showing something approaching zero but it is not as clear as it could have been. This is because of the processing limitations of Google Colab. Using much higher precision would take a lot more time, which Colab does not like. In any case, the results prove that the value would approach zero the more time and precision you throw at it."
      ]
    }
  ]
}