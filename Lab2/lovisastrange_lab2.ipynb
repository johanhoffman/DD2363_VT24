{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Iterative methods**\n",
        "**Lovisa Strange**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "In this report, some iterative algorithms are presented. First, two variants of the same algorithm to solve a system of linear equations are presented. Then, a method for solving a non-linear scalar equation is presented. Additionally, these methods are tested and the result is presented.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88278ec0-83c6-4203-8268-ba8d85807e52"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2024 Lovisa Strange (lstrange@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "The modules needed to run this code is presented here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "Iterative methods are methods that compute an approximate solution in each step. Typically, the algorithm runs until a ceratain number of iterations have been made or until an error approximation or similar is small enough. This means that we get a worse approximation if we only let the algorithm run for a few iterations. On the other hand, if the approximate solution does not need to be precise, for example if it will be used as a starting value for another algorithm, they save a lot of computing power that an exact solving method would have to use.\n",
        "\n",
        "An iterative method has a rate of convergence $q$ if $$\n",
        "lim_{k \\to \\inf} \\frac{||x-x^{(k+1)}||}{||x-x^{(k)}||^q} = C\n",
        "$$\n",
        "for $C>0.$ The rate of convergence is a mesure on how quickly a method converges, so a large value of $q$ means that it converges more quickly.\n",
        "\n",
        "It also important to be able to approximate the error of an iterative method, for example to know when to stop iterating. If the exact solution is known, we can compare the result with the known solution. However, most of the time, the exact solution is not known. Then we can use the residual, which can be computed by putting the computed solution $x^{(k)}$ into the original equation. For a system of equations, we get that $$\n",
        "r^{(k)} = b - Ax^{(k)}\n",
        "$$\n",
        "\n",
        "In this report, one iterative method for nonlinear equations and two iterative methods for linear systems will be presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "One iterative method that can be used for linear systems of equations is Richardson iteration (Methods in Computational Science, p.149). It is a form of fixed point iteration (Methods in Computational Science, p.148), which is a method of the form $$\n",
        "x^{(k+1)} = g(x^{(k)}),\n",
        "$$\n",
        "which solves the equation $x = g(x).$ More specifically, we can use a fixed point iteration method for linear systems, where we have $$g(x) = Mx +c$$ where $M \\in R^{n \\times n}$ and $c âˆˆ R^n.$\n",
        "\n",
        "For Richardson iteration, we can solve the system $Ax=b$ by setting $M=I-\\alpha A$ and $c = \\alpha b,$ $\\alpha$ a real number. We get the iteration $$\n",
        "x^{(k+1)} = (I-\\alpha A)x^{(k)}+ \\alpha b.\n",
        "$$\n",
        "Additionally, we know that Richardson iteration converges if $$\n",
        "||I-\\alpha A||<1.\n",
        "$$\n",
        "This factor also is an upper limit for how fast the method converges for a given A. So, to increase the rate of convergence, we want to make $||I-\\alpha A||$ as small as possible, but still getting the same system. To do this, we can (left) precondition this system. We can use a matrix $B \\approx A^{-1}$ so that  $$BAx = Bb.$$ Then, the rate of convergence is bounded by $$||I-\\alpha BA||.$$   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi iteration\n",
        "\n",
        "One variant of preconditioned Richardson iteration is using the Jacobi preconditioner (Methods in Computational Science, p.150). Here we use the matrix $$\n",
        "B = D^{-1},\n",
        "$$\n",
        "where $D$ is the diagonal matrix created by taking the diagonal from A, and $\\alpha = 1 $.\n",
        "\n",
        "The Jacobi iteration is guaranteed to converge if A is diagonally dominant, that is if the diagonal element on each row is at least as big as the sum of the rest of the elements on that row.\n",
        "\n",
        "An alternative way to formulate Jacobi iteration is by matrix splitting (Methods in Computational Science, p.151). In this case, we spltit $A$ into two parts by $$\n",
        "A = A_1 + A_2 = D + (A-D)\n",
        "$$\n",
        "where D is as above. We then get $$\n",
        "M = -D^{-1}(A-D) = I-D^{-1}A\n",
        "$$\n",
        "and $$\n",
        "c = D^{-1}b.\n",
        "$$\n",
        "\n",
        "The convergence criteria is $$\n",
        "||I-D^{-1}A|| <1.\n",
        "$$\n",
        "Below, the code for constructing the inverse of the diagonal matrix D constructed from an arbitrary non-singular matrix A."
      ],
      "metadata": {
        "id": "AlinTNYwmlyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Inverse of diagonal matrix created from matrix A\n",
        "\n",
        "# Input: Matrix A\n",
        "# Output: Inverse of diagonal of A\n",
        "\n",
        "def diag_inverse(A):\n",
        "\n",
        "  D_inv = np.zeros(A.shape)\n",
        "\n",
        "  for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[0]):\n",
        "      if i==j:\n",
        "        D_inv[i,j] = 1/A[i,j]\n",
        "\n",
        "  return D_inv\n",
        "\n"
      ],
      "metadata": {
        "id": "9w1xr008XLLk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the code for the Jacobi iteration can be found below."
      ],
      "metadata": {
        "id": "dg1QmmkUMXsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Jacobi iteration, partly based on Algorithm 7.1 (Richardson iteration), p.149, Methods in Computational Science\n",
        "\n",
        "# Input: Matrix A, vector b\n",
        "# Output: Vector x so that Ax = b\n",
        "\n",
        "def jacobi_iteration(A,b):\n",
        "  x = np.zeros(b.shape)\n",
        "  tolerance = 10**(-16) # tolerance of algorithm\n",
        "\n",
        "  D_inv = diag_inverse(A)\n",
        "\n",
        "  BA = np.matmul(D_inv,A)\n",
        "  Bb = np.matmul(D_inv,b)\n",
        "\n",
        "  r = 1\n",
        "\n",
        "  while np.linalg.norm(r)/np.linalg.norm(Bb) > tolerance:\n",
        "    r = np.matmul(BA,x)\n",
        "\n",
        "    r[:] = Bb[:]-r[:]\n",
        "\n",
        "    x[:] = x[:] + r[:]\n",
        "\n",
        "    print(\"||Ax-b|| = \",np.linalg.norm(np.matmul(A,x)-b))\n",
        "\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "2cMTaW8Nmo7r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gauss Seidel iteration"
      ],
      "metadata": {
        "id": "KiFbi4aBmpqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another iterative method based on Richardsson iteration is Gauss Seidel iteration (Methods in Computational Science, p.151). Similarly to Jacobi iteration, this method can be seen as a version of preconditioned Richardson iteration. Here, we do an incomplete LU-factorisation, so that $$\n",
        "A \\approx LU.\n",
        "$$\n",
        "Then, we can precondition the system with $$\n",
        "B = U^{-1} L^{-1}\n",
        "$$\n",
        "and use $\\alpha = 1$ to improve the rate of convergence as before.\n",
        "\n",
        "As before, we can also view the Gauss Seidel iteration as the splitting (Methods in Computational Science, p.152) $$\n",
        "A = A_1 + A_2 = L + (A-L).\n",
        "$$\n",
        "Then, we get the matrix $$\n",
        "M = -L^{-1}(A-L) = I-L^{-1}A\n",
        "$$\n",
        "and the vector\n",
        "\n",
        "$$\n",
        "c=L^{-1}b.\n",
        "$$\n",
        "The algorithm converges if $$\n",
        "||I-L^{-1}A||.\n",
        "$$\n",
        "\n",
        "Below, the algorithm for forward substitution to solve the system $$\n",
        "Lx = b\n",
        "$$\n",
        "to find the effect of $L^{-1}$ on a vector $b.$"
      ],
      "metadata": {
        "id": "7Ga4eMyCMuXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward substitution (based on algorithm from Lecture 1, slide 55)\n",
        "\n",
        "## Input: Lower triangular matrix L, vector b\n",
        "## Output: solution to Lx = b\n",
        "\n",
        "def forward_substitution(L,b):\n",
        "  n = len(b)\n",
        "  x = np.zeros(n)\n",
        "  x[0] = b[0]/L[0,0]\n",
        "  for i in range(1,n):\n",
        "    sum = 0\n",
        "    for j in range(0, i-1):\n",
        "      sum += L[i,j]*x[j]\n",
        "    x[i] = (b[i]-sum)/L[i,i]\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "u7gd-5X_ui-H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the lower triangular part L of a matrix A is constructed. Also, the forward-substitution algorithm from above is used to find how the inverse of L acts on a vector b."
      ],
      "metadata": {
        "id": "iUbNUz7wQC0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Inverse of lower triangular matrix made from A\n",
        "\n",
        "# Input: Matrix A, vector B\n",
        "# Output: Inverse of L, lower triangular part of A\n",
        "\n",
        "def lower_inverse_on_vector(A,b):\n",
        "\n",
        "  L = np.zeros(A.shape)\n",
        "  for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[0]):\n",
        "      if j<= i:\n",
        "        L[i,j] = A[i,j]\n",
        "  L_inverse_b = forward_substitution(L,b)\n",
        "  return L_inverse_b"
      ],
      "metadata": {
        "id": "6esQ0Rrn07v7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the Gauss Seidel iteration algorithm is presented, using the algorithm from above."
      ],
      "metadata": {
        "id": "5hM-86elQcD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Gauss Seidel iteration, partly based on Algorithm 7.1 (Richardson iteration), p.149, Methods in Computational Science\n",
        "\n",
        "# Input: Matrix A, vector b\n",
        "# Output: Vector x so that Ax=b\n",
        "\n",
        "def gauss_seidel_iteration(A,b):\n",
        "  x = np.zeros(b.shape)\n",
        "  tolerance = 10**(-16)\n",
        "\n",
        "  BA = np.zeros(A.shape)\n",
        "\n",
        "  for col in range(A.shape[0]):\n",
        "    BA[:,col] = lower_inverse_on_vector(A,np.transpose(A)[col])\n",
        "\n",
        "  Bb = lower_inverse_on_vector(A,b)\n",
        "\n",
        "  r = 1\n",
        "\n",
        "  while np.linalg.norm(r)/np.linalg.norm(Bb) > tolerance:\n",
        "    r = np.matmul(BA,x)\n",
        "\n",
        "    r[:] = Bb[:]-r[:]\n",
        "\n",
        "    x[:] = x[:] + r[:]\n",
        "    print(\"||Ax-b|| = \",np.linalg.norm(np.matmul(A,x)-b))\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "rMW9aAsomuV2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method for scalar nonlinear equation"
      ],
      "metadata": {
        "id": "Q5vqUdNfmurz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve a scalar non-linear equation $$\n",
        "f(x) = 0,\n",
        "$$\n",
        "one methods can be used is Newton's method. (Methods in Computational Science, p. 174)\n",
        "\n",
        "Newtons method is a fixed point method. A general fixed point iteration can be written as  $$\n",
        "x^{(k+1)} = g(x^{(k)}) = x^{(k)} + \\alpha f(x^{(k+1)}).\n",
        "$$\n",
        "To get Newtons method, we chose $$\n",
        "f(x) = - 1/f'(x),\n",
        "$$\n",
        "which leads to a quadratic rate of convergence.\n",
        "\n",
        "The method can be viewed as creating the tangent in a point of the function, and using that point as the new approximation.\n",
        "\n",
        "Below, a non-linear function as well as Newtons method is presented."
      ],
      "metadata": {
        "id": "GgzuGxtoQtJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Newtons method, based on Algorithm 8.2, p.174, Methods in Computational Science\n",
        "\n",
        "# Input: Function f, starting guess x0\n",
        "# Output: Approximate root x\n",
        "\n",
        "def newtons_method(f,x0):\n",
        "  x = x0\n",
        "  tolerance = 10**(-16)\n",
        "\n",
        "  while abs(f(x,\"func\")) > tolerance: # f(x,\"func\") gives f(x)\n",
        "\n",
        "    x = x - f(x,\"func\")/f(x,\"derivative\") # f(x,\"derivative\") gives f'(x)\n",
        "    print(\"|f(x)|= \", f(x,\"func\"))\n",
        "\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "3CMDOGbbm0w6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**\n",
        "In this section, the results from the algorithms presented in the last section are shown."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi iteration\n",
        "\n",
        "To test the convergence of Jacobi iteration, we create diagonally dominant matrix $$\n",
        "A= \\begin{bmatrix}\n",
        "5&0&-1\\\\-2&10&1\\\\0&3&8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "and a vector\n",
        "$$b = \\begin{bmatrix}\n",
        "1\\\\1\\\\0\n",
        "\\end{bmatrix}$$\n",
        "By printing the residual $$\n",
        "||Ax-b||\n",
        "$$in each iteration, we can se that it converges to 0, until the tolerance in the algorithm is reached.\n",
        "\n",
        "By constructing an exact solution $y$, we can also see if the norm of the resulting vector tends to 0."
      ],
      "metadata": {
        "id": "cVQaD6HGZU6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[5,0, -1],[-2,10,1],[0,3,8]]) # diagonally dominant\n",
        "b = np.array([1,1,0])\n",
        "\n",
        "x = jacobi_iteration(A,b)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"x = \", x)\n",
        "exact_solution = np.array([74/391,56/391,-21/391])\n",
        "\n",
        "print(\"|x-y| = \", np.linalg.norm(x-exact_solution))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzkCdv1zZTRS",
        "outputId": "82506907-12f1-40d5-8b4a-c042f2852c08"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax-b|| =  0.5\n",
            "||Ax-b|| =  0.1311964176340193\n",
            "||Ax-b|| =  0.01875000000000009\n",
            "||Ax-b|| =  0.004804172990744791\n",
            "||Ax-b|| =  0.00148850084501993\n",
            "||Ax-b|| =  0.00029635696990848405\n",
            "||Ax-b|| =  7.349674513485788e-05\n",
            "||Ax-b|| =  1.5855122835260745e-05\n",
            "||Ax-b|| =  4.7670132223313976e-06\n",
            "||Ax-b|| =  1.463960843238776e-06\n",
            "||Ax-b|| =  2.867704562348136e-07\n",
            "||Ax-b|| =  1.01327219420185e-07\n",
            "||Ax-b|| =  3.1087240179759763e-08\n",
            "||Ax-b|| =  7.72849394225748e-09\n",
            "||Ax-b|| =  2.5517653426723526e-09\n",
            "||Ax-b|| =  7.368803402755469e-10\n",
            "||Ax-b|| =  2.1018753053018427e-10\n",
            "||Ax-b|| =  6.54915453505514e-11\n",
            "||Ax-b|| =  1.8807016092336133e-11\n",
            "||Ax-b|| =  5.594787020055707e-12\n",
            "||Ax-b|| =  1.68660737728474e-12\n",
            "||Ax-b|| =  4.915859068392439e-13\n",
            "||Ax-b|| =  1.4705430982842269e-13\n",
            "||Ax-b|| =  4.374799983577502e-14\n",
            "||Ax-b|| =  1.2666277535224123e-14\n",
            "||Ax-b|| =  4.010656666373001e-15\n",
            "||Ax-b|| =  9.104505742017336e-16\n",
            "||Ax-b|| =  4.871083751574258e-16\n",
            "||Ax-b|| =  2.220446049250313e-16\n",
            "||Ax-b|| =  2.220446049250313e-16\n",
            "\n",
            "x =  [ 0.18925831  0.14322251 -0.05370844]\n",
            "|x-y| =  2.8609792490763984e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By loking at the numerical result, we can see that the residual goes to 0. We can also see that the difference between the exact and numerical solution is smaller than the tolerance level of the algorithm, and is very close to 0."
      ],
      "metadata": {
        "id": "4l6oHRpSpZuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gauss Seidel iteration\n",
        "\n",
        "To test the convergence of Jacobi iteration, we create diagonally dominant matrix $$\n",
        "A= \\begin{bmatrix}\n",
        "6&0&1\\\\3&50&1\\\\0&2&3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "and a vector\n",
        "$$b = \\begin{bmatrix}\n",
        "1\\\\2\\\\0\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "Again, we construct the residual $$\n",
        "||Ax-b||\n",
        "$$\n",
        "and seeing if this converges to 0, and also look at the difference between the exact solution and the computed one."
      ],
      "metadata": {
        "id": "C1Xt8hHSlWbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[6,0, 1],[3,5,1],[0,2,3]]) # diagonally dominant\n",
        "b = np.array([1,2,0])\n",
        "\n",
        "x = gauss_seidel_iteration(A,b)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"x = \", x)\n",
        "exact_solution = np.array([17/84,9/28,-3/14])\n",
        "\n",
        "print(\"|x-y| = \", np.linalg.norm(x-exact_solution))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stW2UrD3ZT17",
        "outputId": "670fe5a0-e102-4591-87e4-71d5b300d4a2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Ax-b|| =  0.9433981132056605\n",
            "||Ax-b|| =  0.42687494916218977\n",
            "||Ax-b|| =  0.2362672686222582\n",
            "||Ax-b|| =  0.11139962541772676\n",
            "||Ax-b|| =  0.05870032704390762\n",
            "||Ax-b|| =  0.030124752046631627\n",
            "||Ax-b|| =  0.015062842298417211\n",
            "||Ax-b|| =  0.007893255230152725\n",
            "||Ax-b|| =  0.004011512893632189\n",
            "||Ax-b|| =  0.0020546469088300453\n",
            "||Ax-b|| =  0.0010603179739570208\n",
            "||Ax-b|| =  0.0005412314635050782\n",
            "||Ax-b|| =  0.00027833052232868803\n",
            "||Ax-b|| =  0.00014284414506531882\n",
            "||Ax-b|| =  7.318967147645076e-05\n",
            "||Ax-b|| =  3.760060339760802e-05\n",
            "||Ax-b|| =  1.9281475952110146e-05\n",
            "||Ax-b|| =  9.892693386252103e-06\n",
            "||Ax-b|| =  5.0775574335642285e-06\n",
            "||Ax-b|| =  2.604454790005896e-06\n",
            "||Ax-b|| =  1.3365201758124283e-06\n",
            "||Ax-b|| =  6.857643396087107e-07\n",
            "||Ax-b|| =  3.5183295668104024e-07\n",
            "||Ax-b|| =  1.8053657887626318e-07\n",
            "||Ax-b|| =  9.262868218433063e-08\n",
            "||Ax-b|| =  4.752707347781646e-08\n",
            "||Ax-b|| =  2.438626251892556e-08\n",
            "||Ax-b|| =  1.2512188441740704e-08\n",
            "||Ax-b|| =  6.4199730121574435e-09\n",
            "||Ax-b|| =  3.2940424809248084e-09\n",
            "||Ax-b|| =  1.69014223953316e-09\n",
            "||Ax-b|| =  8.672034753790133e-10\n",
            "||Ax-b|| =  4.4495522879676714e-10\n",
            "||Ax-b|| =  2.2830352788024107e-10\n",
            "||Ax-b|| =  1.171410812525168e-10\n",
            "||Ax-b|| =  6.010399621467032e-11\n",
            "||Ax-b|| =  3.0838863093433944e-11\n",
            "||Ax-b|| =  1.5823425889432878e-11\n",
            "||Ax-b|| =  8.119011522353493e-12\n",
            "||Ax-b|| =  4.165833447627357e-12\n",
            "||Ax-b|| =  2.1377313680792177e-12\n",
            "||Ax-b|| =  1.0967382541856454e-12\n",
            "||Ax-b|| =  5.630266239061644e-13\n",
            "||Ax-b|| =  2.885563951282221e-13\n",
            "||Ax-b|| =  1.4834927272743767e-13\n",
            "||Ax-b|| =  7.600760367900053e-14\n",
            "||Ax-b|| =  3.8765706809576265e-14\n",
            "||Ax-b|| =  2.0194541777021977e-14\n",
            "||Ax-b|| =  1.030509615489028e-14\n",
            "||Ax-b|| =  5.016930660992626e-15\n",
            "||Ax-b|| =  2.8522145930998398e-15\n",
            "||Ax-b|| =  1.227537204523176e-15\n",
            "||Ax-b|| =  6.13768602261588e-16\n",
            "||Ax-b|| =  1.3877787807814457e-16\n",
            "||Ax-b|| =  4.577566798522237e-16\n",
            "||Ax-b|| =  8.326672684688674e-17\n",
            "||Ax-b|| =  0.0\n",
            "\n",
            "x =  [ 0.20238095  0.32142857 -0.21428571]\n",
            "|x-y| =  2.7755575615628914e-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can see that the residual goes to 0. Additionally, the difference between the exact and numerical solution is very small."
      ],
      "metadata": {
        "id": "82Ak_Im8p7Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's method for scalar nonlinear equation\n",
        "\n",
        "We have the non-linear function $$\n",
        "f(x) = x^2+x-12\n",
        "$$\n",
        "and similarly to the two previous cases, we look at the residual $$\n",
        "|f(x)|\n",
        "$$\n",
        "which should go to 0.\n",
        "\n",
        "Additionally, we construct an exact solution $y $ and compute $|x-y|$"
      ],
      "metadata": {
        "id": "CcV5bBSMoKVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x,type):\n",
        "  if type == \"derivative\":\n",
        "    return 2*x +1\n",
        "  else:\n",
        "    return x**2+x-12"
      ],
      "metadata": {
        "id": "uCwVvaMXzolI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = newtons_method(f,2)\n",
        "\n",
        "exact_solution = 3\n",
        "print(\"|x-y| = \", x-exact_solution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS3JUruOZUlU",
        "outputId": "f2d05c75-95ad-470c-fb7c-852f1eec36e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|f(x)|=  1.4400000000000013\n",
            "|f(x)|=  0.03786705624543352\n",
            "|f(x)|=  2.917336959562533e-05\n",
            "|f(x)|=  1.737099353249505e-11\n",
            "|f(x)|=  0.0\n",
            "|x-y| =  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "We can see that the residual goes to 0, as well as that the difference between $x$ and $y$ is very small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "All three of the methods behaved as expected. They all converged to the exact solution. Also, depending on the tolerance set, the computed result was closer or further away from the exact solution. We can also see that Newton's method converged quicker than the other methods, which is expected since the method has a quadratic rate of convergence."
      ]
    }
  ]
}