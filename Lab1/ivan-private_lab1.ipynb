{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT24/blob/ivan-private-Lab1/Lab1/ivan-private_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RgtXlfYO_i7"
   },
   "source": [
    "# **Lab 1: Matrix factorization**\n",
    "**Ivan Zivkovic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x_J5FVuPzbm"
   },
   "source": [
    "# **Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UFTSzW7P8kL"
   },
   "source": [
    "In this first lab of the course we delve deeper into some useful matrix algorithm, such as sparse matrix-vector product and QR factorization. The algorithms implemented in this lab are good base algorithms that operate on matrices in different ways and can probably be reused as subroutines in many other algorithms. \n",
    "\n",
    "Under the method chapter, the implementations and descriptions of the algorithms are explained. In the results chapter the different implementations are tested to measure correctness and approximation error size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJipbXtnjrJZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkT8J7uOWpT3"
   },
   "source": [
    "# **About the code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmB2noTr1Oyo"
   },
   "source": [
    "This report is written by Ivan Zivkovic (ivanzi@kth.se)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pdll1Xc9WP0e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "a790b00c-cbec-47f1-9059-e0be7f88ff5d",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.814052Z",
     "start_time": "2024-01-28T19:03:26.780660Z"
    }
   },
   "source": [
    "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
    "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
    "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
    "\n",
    "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
    "\n",
    "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
    "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
    "#\n",
    "# This is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# This template is maintained by Johan Hoffman\n",
    "# Please report problems to jhoffman@kth.se"
   ],
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28xLGz8JX3Hh"
   },
   "source": [
    "# **Set up environment**"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load neccessary modules.\n",
    "#from google.colab import files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import Latex\n",
    "\n",
    "from typing import TypeAlias\n",
    "T_NumpyVector: TypeAlias = np.ndarray\n",
    "T_NumpyMatrix: TypeAlias = np.ndarray"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.847522Z",
     "start_time": "2024-01-28T19:03:26.814749Z"
    }
   },
   "execution_count": 250
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnO3lhAigLev"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5zMzgPlRAF6"
   },
   "source": [
    "In this lab we will investigate and implement a couple of algorithms that deal with matrices. The first problem is the sparse matrix-vector product which combines how to store sparse matrices and also an algorithm to multiply one with a vector. The second problem we will deal with is how to implement a QR factorization of a matrix where Q is an orthonormal matrix and R is an upper triangular matrix such that $A = QR$. The third problem dealt with in this lab is an algorithm for a direct solver for the equation $Ax = b$ which can be solved by $x = A^{-1} b$. Lastly in this lab we will implement an algorithm called \"QR eigenvalue algorithm\" which, given a symmetric matrix A, outputs its eigenvalues and eigenvectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOQvukXZq5U5"
   },
   "source": [
    "# **Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Sparse matrix-vector product**"
   ],
   "metadata": {
    "id": "eONxFKymDTDt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A sparse matrix is a matrix that has relatively few non-zero elements. When the matrix is sparse, you could save on memory by saving the matrix in a different datastructure than the typical \"two dimensional\" array. One example of such a datastructure is a compressed sparse row matrix (CSR matrix). This way of storing a matrix consists of three lists. \n",
    "1. The list \"val\" stores all the non-zero values. \n",
    "2. The list \"col_idx\" stores in which column the corresponding element in \"val\" is located (same index).\n",
    "3. The list \"row_ptr\" stores the index in \"val\" and \"col_idx\" for where a new row starts.\n",
    "\n",
    "The CSR matrix is implemented here as a class that takes an $m \\times n$ numpy array as input and converts it into a CSR matrix. \n",
    "\n",
    "The function for the sparse matrix-vector product is implemented here in the \"sparse_matrix_vector_product\" function. How it works in a high level is that the algorithm iterates through the sparse matrix and only performs multiplications with the vector x where the corresponding element in A is non-zero. This is possible since the column and row indices of each non-zero element is saved in the datastructure and in total a lot fewer multiplications are carried out. \n",
    "\n",
    "If we assume that there are $O(n)$ elements in the sparse matrix to begin with, this sparse matrix-vector product has a time complexity of $O(n^2)$. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SparseMatrix:\n",
    "    '''\n",
    "        :param A: dense matrix of shape (m, n)\n",
    "        :return SparseMatrix\n",
    "    '''\n",
    "    def __init__(self, A: T_NumpyMatrix):\n",
    "        m, n = A.shape\n",
    "        val = []\n",
    "        col_idx = []\n",
    "        row_ptr = []\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if A[i, j] != 0:\n",
    "                    if len(row_ptr) == i:\n",
    "                        row_ptr.append(len(val))\n",
    "                    val.append(A[i, j])\n",
    "                    col_idx.append(j)\n",
    "\n",
    "        row_ptr.append(len(val))\n",
    "        self.val = np.array(val)\n",
    "        self.col_idx = np.array(col_idx)\n",
    "        self.row_ptr = np.array(row_ptr)\n",
    "\n",
    "\n",
    "def sparse_matrix_vector_product(A: SparseMatrix, x: T_NumpyVector) -> T_NumpyVector:\n",
    "    n = x.shape[0]\n",
    "    b = np.zeros((n,))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(A.row_ptr[i], A.row_ptr[i+1]):\n",
    "            b[i] = b[i] + A.val[j] * x[A.col_idx[j]]\n",
    "\n",
    "    return b"
   ],
   "metadata": {
    "id": "hwwxa7EqD7zu",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.853575Z",
     "start_time": "2024-01-28T19:03:26.840753Z"
    }
   },
   "execution_count": 251,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **QR factorization**"
   ],
   "metadata": {
    "id": "t7UAsWfBDYgV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "QR factorization is an algorithm that factors a nonsingular matrix $A$ into an orthonormal matrix $Q$ and upper triangular matrix $R$ such that $A = QR$. The purpose is that both the $Q$ and $R$ matrices are easy to invert, making it easy to invert $A$. \n",
    "\n",
    "The method used for this QR factorization is called the modified Gram-Schmidt QR factorization and in a high level it works as the following:\n",
    "For each new column vector $q_{:j}$, start with the corresponding column vector $a_{:j}$ and subtract the projection of every already established column vector $q_{:i}, \\; 0 \\leq i \\leq j-1$. Also for each of these subtractions, save the inner product between $q_{:i}$ and $a_{:j}$ in $r_{ij}$. Amd lastly after each projection has been subtracted off, normalize this vector and save it as $q_{:j}$, and also save this norm in $r_{jj}$. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def modified_gram_schmidt_QR_factorization(A: T_NumpyMatrix) -> tuple[T_NumpyMatrix, T_NumpyMatrix]:\n",
    "    _, n = A.shape\n",
    "    Q = np.zeros((n, n))\n",
    "    R = np.zeros((n, n))\n",
    "\n",
    "\n",
    "    for j in range(n):\n",
    "        v = A[:, j]\n",
    "\n",
    "        for i in range(j):\n",
    "            R[i, j] = Q[:, i].T @ v\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "\n",
    "        R[j, j] = np.sqrt(v.T @ v) #norm of v\n",
    "\n",
    "        Q[:, j] = v / R[j, j]\n",
    "\n",
    "    return Q, R"
   ],
   "metadata": {
    "id": "0SyItsDBEP9q",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.873048Z",
     "start_time": "2024-01-28T19:03:26.856793Z"
    }
   },
   "execution_count": 252,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Direct solver Ax=b**"
   ],
   "metadata": {
    "id": "yhoBdK59Dc5T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A function for directly solving the equation $Ax = b$ by calculating $x = A^{-1}b$. The method for solving this equation is to first decompose A into a QR factorization and then taking the inverse of Q and R:\n",
    "$$Ax = QR x= b$$\n",
    "$$x = R^{-1} Q^{-1} b$$\n",
    "\n",
    "The inverse of Q is the same as its transpose, and the inverse of R can be calculated through a backwards substitution which will also be implemented. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def backwards_substitution(R: T_NumpyMatrix, b: T_NumpyVector) -> T_NumpyVector:\n",
    "    # backwards substitution algorithm\n",
    "    _, n = R.shape\n",
    "    x = np.zeros(n)\n",
    "    x[n-1] = b[n-1] / R[(n-1, n-1)]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = b[i]\n",
    "        for j in range(i+1, n):\n",
    "            x[i] = x[i] - R[i, j] * x[j]\n",
    "        x[i] = x[i] / R[i, i]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def direct_solver(A: T_NumpyMatrix, b: T_NumpyVector) -> T_NumpyVector:\n",
    "\n",
    "    Q, R = modified_gram_schmidt_QR_factorization(A)\n",
    "\n",
    "    # Q.T = Q^-1 since Q is orthogonal\n",
    "    return backwards_substitution(R, Q.T @ b)"
   ],
   "metadata": {
    "id": "v5oD-fx7EahR",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.876568Z",
     "start_time": "2024-01-28T19:03:26.860205Z"
    }
   },
   "execution_count": 253,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **QR eigenvalue algorithm**"
   ],
   "metadata": {
    "id": "QzB9yYaJDwTI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An algorithm to easily calculate the eigenvalues and eigenvectors for a symmetric matrix $A$ is the QR eigenvalue algorithm. The goal of the algorithm is to approximate the eigendecomposition $A = U \\Lambda U^*$ where $\\Lambda$ is a diagonal matrix with the eigenvalues of $A$ on its diagonal and $U$ is a unitary matrix with the corresponding eigenvectors to the eigenvalues as its columns. If however $A$ is not a symmetric matrix, then the algorithm is going to approximate a Schur factorization $A = U T U^*$ where $T$ is an upper triangular matrix with the eigenvalues of $A$ on the diagonal, and $U$ does not necessarily approximate the eigenvectors. \n",
    "\n",
    "The way that this algorithm works in a high level is that the QR factorization algorithm is run in multiple iterations on the matrix $A^{(k-1)}$, where after each iteration the matrix $A^{(k)} = R^{(k-1)} Q^{(k-1)}$ and $U^{(k)} = U^{(k-1)} Q^{(k-1)}$. As $k \\rightarrow \\infty$, the matrix $A^{(k)}$ will converge to either $\\Lambda$ or $T$ depending on if $A$ is symmetric or not. In this practical implementation, $k$ will be an input parameter to the function that dictates how many iterations the QR factorization will be re-run for. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def qr_algorithm(A: T_NumpyMatrix, k: int) -> tuple[T_NumpyMatrix, T_NumpyMatrix]:\n",
    "    n = A.shape[0]\n",
    "    U = np.identity(n)\n",
    "\n",
    "    for i in range(k):\n",
    "        Q, R = modified_gram_schmidt_QR_factorization(A)\n",
    "        A = R @ Q\n",
    "        U = U @ Q\n",
    "\n",
    "    return A, U\n"
   ],
   "metadata": {
    "id": "NTGUHn1OEd2N",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.876781Z",
     "start_time": "2024-01-28T19:03:26.862819Z"
    }
   },
   "execution_count": 254,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsQLT38gVbn_"
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLwlnOzuV-Cd"
   },
   "source": [
    "Present the results. If the result is an algorithm that you have described under the *Methods* section, you can present the data from verification and performance tests in this section. If the result is the output from a computational experiment this is where you present a selection of that data."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Sparse matrix-vector product**"
   ],
   "metadata": {
    "id": "77mpJZUDEpXy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To verify that the results of the sparse matrix-vector product is within a reasonable margin of error, the following test will be conducted. A random dense matrix $A$ will be generated and from this matrix a sparse matrix will be created. Then both the numpy matrix product and the implemented sparse matrix-vector product will be evaluated with a randomly generated vector $x$. \n",
    "\n",
    "With a high probability the random matrix $A$ will not be sparse, but this test aims to test the correctness so it is fine. \n",
    "\n",
    "The results of the test will be the output vector of numpy's matrix product, the output vector of the sparse matrix vector product and the norm of the difference between the two vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n = 6\n",
    "A = np.random.randint(0, 10, size=(n,n))\n",
    "x = np.random.randint(0, 10, size=(n,))\n",
    "sparse_A = SparseMatrix(A)\n",
    "\n",
    "\n",
    "Ax = A @ x\n",
    "sparse_Ax = sparse_matrix_vector_product(sparse_A, x)\n",
    "diff = np.linalg.norm(Ax - sparse_Ax)\n",
    "\n",
    "print(f\"Randomly generated A:\\n{A}\")\n",
    "print(f\"Randomly generated x: {x}\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"A * x: {Ax}\")\n",
    "print(f\"sparse_A * x: {sparse_Ax}\")\n",
    "print(f\"norm of difference: {diff}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjOaGUtrFb01",
    "outputId": "8a8dccee-bd75-4524-a7f2-0891bd599c3a",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.882321Z",
     "start_time": "2024-01-28T19:03:26.877055Z"
    }
   },
   "execution_count": 255,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly generated A:\n",
      "[[8 8 6 8 7 4]\n",
      " [5 4 6 6 3 2]\n",
      " [2 5 9 3 7 9]\n",
      " [6 1 5 8 1 2]\n",
      " [6 4 5 0 1 2]\n",
      " [2 1 5 1 1 9]]\n",
      "Randomly generated x: [6 5 4 6 7 4]\n",
      "\n",
      "Results:\n",
      "A * x: [225 139 176 124  91  86]\n",
      "sparse_A * x: [225. 139. 176. 124.  91.  86.]\n",
      "norm of difference: 0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **QR factorization**"
   ],
   "metadata": {
    "id": "vCajXVOmEtYX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result of the QR factorization will be measured by checking if the implementation produces a correct factorization of $A = QR$. This will be tested in the three following ways:\n",
    "\n",
    "1. Firstly we will check if R is an upper triangular matrix by comparing it with the output of np.triu(R) which gives back R as an upper triangular matrix, so it should be equal to R. \n",
    "2. The Frobenius norm $||Q^T Q - I||_F$, which if Q is an orthonormal matrix should be equal to 0 (or close to 0 because of float finite precision).\n",
    "3. The Frobenius norm $||QR-A ||_F$, which should be equal to 0 (or close) since it should be so that $A = QR$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n = 2\n",
    "A = np.array([\n",
    "    [2, -1],\n",
    "    [-1, 2]\n",
    "])\n",
    "\n",
    "Q, R = modified_gram_schmidt_QR_factorization(A)\n",
    "\n",
    "\n",
    "print(\"Test 1:\")\n",
    "if np.allclose(R, np.triu(R)):\n",
    "    print(\"R is upper triangular\")\n",
    "else:\n",
    "    print(\"R is not upper triangular\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest 2:\")\n",
    "frob_norm = np.linalg.norm((Q.T @ Q) - np.identity(n))\n",
    "display(Latex(f'$||Q^T Q - I||_F = {frob_norm}$'))\n",
    "\n",
    "\n",
    "print(\"\\nTest 3:\")\n",
    "frob_norm = np.linalg.norm((Q @ R) - A)\n",
    "display(Latex(f'$||QR-A ||_F = {frob_norm}$'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5tJw4L-FlHt",
    "outputId": "307bbe9a-e65d-4bbd-d173-a48f0da5d75a",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.883907Z",
     "start_time": "2024-01-28T19:03:26.880949Z"
    }
   },
   "execution_count": 256,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n",
      "R is upper triangular\n",
      "\n",
      "Test 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||Q^T Q - I||_F = 2.6901577681355055e-16$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||QR-A ||_F = 0.0$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Direct solver Ax=b**"
   ],
   "metadata": {
    "id": "eBhXoRFFEvJu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result for the direct solver implementation will be measured by testing the residual (size of error) in the following two ways:\n",
    "1. $||Ax - b||$, which should be 0 (or close)\n",
    "2. $||x-y||$, where $y$ is a manufactured solution with $b=Ay$, which should be 0 (or close)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "A = np.array([\n",
    "    [2, -1],\n",
    "    [-1, 2]\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Test 1:\")\n",
    "b = np.array( [1, 0] )\n",
    "x = direct_solver(A, b)\n",
    "norm = np.linalg.norm( (A @ x) - b )\n",
    "display(Latex(f'$||Ax - b|| = {norm}$'))\n",
    "\n",
    "\n",
    "print(\"\\nTest 2:\")\n",
    "y = np.array( [1, 2] )\n",
    "b = A @ y\n",
    "x = direct_solver(A, b)\n",
    "norm = np.linalg.norm( x - y )\n",
    "display(Latex(f'$||x-y|| = {norm}$'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiHPmKRBFn3E",
    "outputId": "6bd67d10-1e36-44b6-ca85-1a169a0b774e",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.908391Z",
     "start_time": "2024-01-28T19:03:26.885354Z"
    }
   },
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||Ax - b|| = 2.220446049250313e-16$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||x-y|| = 6.280369834735101e-16$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **QR eigenvalue algorithm**"
   ],
   "metadata": {
    "id": "rfJ7CXNhExwa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result for the QR eigenvalue algorithm implementation will be measured by testing the residual (size of error) in the following two ways:\n",
    "1. $det(A - \\lambda_i I)$, which should be 0 (or close)\n",
    "2. $||A v_i - \\lambda_i v_i||$, which should be 0 (or close)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n = 3\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, -1, -1],\n",
    "    [3, -1, 2]\n",
    "])\n",
    "\n",
    "A2, U = qr_algorithm(A, 1000)\n",
    "\n",
    "\n",
    "print(\"Test 1:\")\n",
    "for i in range(n):\n",
    "    lambda_i = A2[i, i]\n",
    "    det = np.linalg.det(A - (lambda_i * np.identity(n)))\n",
    "    display(Latex(f\"$det(A - \\lambda_{i+1}I) = {det}$\"))\n",
    "\n",
    "print(\"\\nTest 2:\")\n",
    "for i in range(n):\n",
    "    lambda_i = A2[i, i]\n",
    "    v_i = U[:, i]\n",
    "    norm = np.linalg.norm( (A @ v_i) - lambda_i * v_i )\n",
    "    display(Latex(f\"$||A v_{i+1} - \\lambda_{i+1} v_{i+1}|| = {norm}$\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YSI6LQ_Fqic",
    "outputId": "12c07ee3-74e3-4c72-a881-7d48fe9d23e8",
    "ExecuteTime": {
     "end_time": "2024-01-28T19:03:26.909826Z",
     "start_time": "2024-01-28T19:03:26.889644Z"
    }
   },
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$det(A - \\lambda_1I) = 4.5327417829133314e-14$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$det(A - \\lambda_2I) = 6.7123998273390665e-15$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$det(A - \\lambda_3I) = -6.138159518165522e-16$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||A v_1 - \\lambda_1 v_1|| = 1.8610729195778454e-15$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||A v_2 - \\lambda_2 v_2|| = 1.2755491433176288e-15$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$||A v_3 - \\lambda_3 v_3|| = 7.631521572941402e-16$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4GLBv0zWr7m"
   },
   "source": [
    "# **Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bcsDSoRXHZe"
   },
   "source": [
    "As seen in the results, many of the approximations differ a little bit from what they are trying to approximate which is expected. In many testcases, the different residual values are in the size order of around $10^{-14}$ to $10^{-16}$ which is very close to 0. A probable reason for these differences could be due to round of errors in the floating point representation of the values used in the different algorithms. However, these differences are so small that they could be considered negligible for some applications. "
   ]
  }
 ]
}
