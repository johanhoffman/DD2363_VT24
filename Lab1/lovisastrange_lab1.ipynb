{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 1: Matrix Factorization**\n",
        "**Lovisa Strange**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "In this lab report, three algorithms related to matrix vector multiplication and matrix factorization are presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2024 Lovisa Strange (lstrange@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common\n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "\n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "When doing computations with large matrices, the time complexity of the computations are important. One area in which this can be considered is when working with sparse matrices, ie matrices that largely consist of zeros. Often, computations with such matrices can become faster if we consider the large amount of 0:s in them.\n",
        "\n",
        "Another way to compute things more efficiently is by factorizing maricies into a form that is easier to invert. When solving a matrix equation $$Ax = b$$, it is expensive to directly invert the matrix A. However, if we can write it as $$A = BC,$$ where $B$ and $C$ is easier to invert, we can solve the equation more efficiently.\n",
        "\n",
        "In this report, an algorithm for multiplicating a sparse matrix and a vector will be presented. Also, a way to factorize matrices will be presented, as well as using the factorization to solve a matrix equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "##Sparse matrix-vector product\n",
        "For matrices that are sparse it is often not efficient to use regular matrix-vector multiplication. Instead, we can use another algoritm that uses a specific data structure to make the computation more efficient.\n",
        "\n",
        "This data structure is called *compressed row structure* (lecture X, slide Y) or CRS. A matrix is represented as 3 arrays. The first one conatins all non-zero elements of the matrix: The seccond one (of the same length) has the column index for each of the non-zero elements. Lastly, the third array contains the index that points to the position in the first array where each new row starts.\n",
        "\n",
        "We look at an example using the matrix\n",
        "$$A= \\begin{bmatrix}3&0&0\\\\0&2&0 \\end{bmatrix}.$$\n",
        "This matrix would be reprecented with the values $$\n",
        "val=[3,2],$$\n",
        "the column indicies $$\n",
        "col\\_idx = [0,1]\n",
        "$$\n",
        "and the row pointers $$\n",
        "row\\_ptr=[0,1,2].\n",
        "$$\n",
        "\n",
        "We can find the CRS representation of a given matrix by looping over its rows and saving the non-zero elements and their columns, as well as keeping track of when a new row is started. This is done in the code below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CRS representation of A\n",
        "\n",
        "class CRS:\n",
        "  def __init__(self,A):\n",
        "    val = []\n",
        "    col_idx = []\n",
        "    row_ptr = []\n",
        "\n",
        "    for row in A:\n",
        "      colCounter = 0\n",
        "      row_ptr.append(len(val))\n",
        "\n",
        "      for elem in row:\n",
        "        if elem != 0:\n",
        "          val.append(elem)\n",
        "          col_idx.append(colCounter)\n",
        "        colCounter +=1\n",
        "    row_ptr.append(len(val))\n",
        "\n",
        "\n",
        "    self.val = val\n",
        "    self.col_idx = col_idx\n",
        "    self.row_ptr = row_ptr"
      ],
      "metadata": {
        "id": "gCWTJPEKC3Vp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can use this data structure to compute the product between a sparse matrix and a vector more efficiently. (Lecture X, slide Y) To understand how this works, we will lok at an example.\n",
        "\n",
        "Using the regular matrix-vector product, we would do the following computations\n",
        "$$c =A\\cdot b = \\begin{bmatrix}3&0&0\\\\0&2&0 \\end{bmatrix} \\begin{bmatrix}1\\\\2\\\\3 \\end{bmatrix}= \\begin{bmatrix}3⋅1+0\\cdot2+0⋅3\\\\0⋅1+2⋅2+0⋅3 \\end{bmatrix} = \\begin{bmatrix}3\\\\4 \\end{bmatrix}$$\n",
        "\n",
        "If we instead store the matrix using CRS, we loop over each row, and then look at the non-zero elements of that row and multiply them with the corresponding element in the vector. We get for the same A as above that for the first row we compute\n",
        "$$\n",
        "c[0] = val[0]⋅b[0]= 3⋅1 =3\n",
        "$$\n",
        "\n",
        "and for the seccond row we get\n",
        "\n",
        "$$\n",
        "c[1] = val[1]⋅b[1] =2⋅2=4.\n",
        "$$\n",
        "\n",
        "This means that we don't have to compute the multiplication with the zeroes, which is useful for sparse matrices.\n",
        "\n",
        "The algorithm for this is presented below."
      ],
      "metadata": {
        "id": "dbJgVj4Hy-_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse matrix vector multiplucation\n",
        "\n",
        "## Input: vector x with length n, CRS representation of mxn matrix A\n",
        "## Output: b = Ax\n",
        "\n",
        "def sparse_matrix_vector_product(A,x):\n",
        "\n",
        "  b= np.empty(len(x)) # result\n",
        "  for i in range(len(x)):\n",
        "    b[i] = 0\n",
        "    for j in range(A.row_ptr[i],A.row_ptr[i+1]):\n",
        "      b[i] += A.val[j]*x[A.col_idx[j]]\n",
        "\n",
        "  return b"
      ],
      "metadata": {
        "id": "ck1hlLiOA3EB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QR factorization\n",
        "\n",
        "One way in which we can factorize matrices is by QR-factorization, $$\n",
        "A = QR,\n",
        "$$\n",
        "where Q is an orthogonal matrix and R is an upper triangular matrix. This can be done by so called Gram-Schmidt QR factorization. (Reference! kap 5.3)\n",
        "\n",
        "We want to construct an orthogonal basis for the matrix A using Gram Schmits method, which is done by going through all the vectors $a_{:j}$ that span $A,$ and suptracting the projection of $a_{:j}$ on all vectors alredy in the orthogonal base from $a_{:j}.$ This is done by computing $$\n",
        "v_j = a_j - ∑_{i=1}^{j-1}(a_{:j}q_i)q_i\n",
        "$$\n",
        "and normalising the result. Writing this as a linear system with a matric R having elements $r_{ij} = (a_{:j}q_i)$ and $r_{ii}=||v_j||, $ we get an upper triangular matrix R, and together with the (orthogonal) columns of Q, this is a QR-factorization of A.\n",
        "\n",
        "However, when doing this nummerically, instead want to use an equivalent form of Gram-Schmidts method, where we construct a projector $P_j$ as $$\n",
        "P_J = P^{\\perp q_{:j-1}}...P^{\\perp q_{:1}}\n",
        "$$ where $$\n",
        "P^{\\perp q_{:i}} = I - q_{:i}q_{:i}^T.\n",
        "$$\n",
        "This is done in the algorithm below"
      ],
      "metadata": {
        "id": "W0uU4h_nAxcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QR - factrorization (based on algorithm 5.3 from the course book, p.89)\n",
        "\n",
        "## Input: Matrix A\n",
        "## Output: Q orthogonal, R upper triangular so that A = QR\n",
        "\n",
        "def QR_factorization(A):\n",
        "  n = A.shape[0]\n",
        "  R= np.zeros([n,n])\n",
        "  Q= np.zeros([n,n])\n",
        "  v = np.zeros(n)\n",
        "\n",
        "  for j in range(n):\n",
        "    v[:] = A[:, j]\n",
        "    for i in range(j):\n",
        "      R[i,j] = np.dot(Q[:,i],v[:])\n",
        "      v[:] = v[:] - R[i,j]*Q[:,i]\n",
        "    R[j,j] = np.sqrt(sum([elem**2 for elem in v]))\n",
        "    Q[:,j] = v[:]/R[j,j]\n",
        "\n",
        "  return Q,R"
      ],
      "metadata": {
        "id": "Qy-oZwF4Q5AV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solving Ax=b\n",
        "\n",
        "Now, we want to use QR-factorisation to solve the matrix equation $$Ax = b \\implies x = A^{-1}b.$$ By factorizing the matrix A, we get A = QR, and the equation can be written as $$x = (QR)^{-1}b = Q^{-1}R^{-1}b.$$ Since both Q and R are of a specific form, we can find the inverse to them more easily than for the matrix A, which we don't know the structure of.\n",
        "\n",
        "We know that Q is a normal matrix, and so its inverse is the same as its transpose, $$Q^T = Q^{-1}.$$ We also know that R is upper triangular, which we can use to solve the equation $$Rx=b$$ one row at a time. This is done by doing so called backwards substitution, where we start from the last row of the matrix and solve for $x_{n-1},$ then use that value to solve for $x_{n-2}$ using the next to last row and so on, until all $x_j$:s have been solved for. The code for the backwards substitution is shown below."
      ],
      "metadata": {
        "id": "eD3bD9pYAzUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward substitution\n",
        "\n",
        "## Input: Upper triangular matrix U, vector b\n",
        "## Output: solution to Ux = b\n",
        "\n",
        "def backward_substitution(U,b):\n",
        "  n = len(b)\n",
        "  x = np.zeros(n)\n",
        "  x[n-1] = b[n-1]/U[n-1,n-1]\n",
        "  for i in reversed(range(n-1)):\n",
        "    sum = 0\n",
        "    for j in range(i+1, n):\n",
        "      sum += U[i,j]*x[j]\n",
        "    x[i] = (b[i]-sum)/U[i,i]\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "NfUD4HyXwYuw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we simply use the QR-factorization from the previous section, and solve the system given by $$Rx = b$$ using backwards substitution. Finally, we find the inverse of $Q,$ and multiply these together to get the final result.  "
      ],
      "metadata": {
        "id": "xhby-r58rnB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solving Ax = b\n",
        "\n",
        "## Input: matrix A, vector b\n",
        "## Output: Solution b\n",
        "\n",
        "def solver(A,b):\n",
        "  Q,R = QR_factorization(A) # x = (QR)^(-1)b = Q^(-1)R^(-1)b\n",
        "\n",
        "  partialSol = backward_substitution(R,b)\n",
        "  QInverse = np.transpose(Q)\n",
        "\n",
        "  x = np.matmul(QInverse,partialSol)\n",
        "\n",
        "  return x\n",
        "\n"
      ],
      "metadata": {
        "id": "trDCXupzvI-_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**\n",
        "In this section, the results produced by the algorithms in the previous section are presented"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sparse matrix-vector product\n",
        "Here, we want to test the computations made by the sparse matrix-vector product with that of regular matrix-vector multiplication. If these are the same, the computations are correct.\n",
        "\n",
        "We look at a vector $x$ and a matrix $A$, and compute the CRS representation of the matrix, and then use the algorithm **sparse_matrix_vector_product** from above to compute the resulting vector.\n",
        "\n",
        "Then, we use the **matmul-**function from the numpy package to compute the regular matrix vector product."
      ],
      "metadata": {
        "id": "JFncQMyJNmDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1,1,1,1,1,1])\n",
        "A = np.array([[3,2,0,2,0,0],[0,2,1,0,0,0],[0,0,1,0,0,0],[0,0,3,2,0,0],[0,0,0,0,1,0],[0,0,0,0,2,3]])\n",
        "A_sparse = CRS(A)\n",
        "\n",
        "b_sparse = sparse_matrix_vector_product(A_sparse,x)\n",
        "print(\"sparse = \",np.array(b_sparse))\n",
        "\n",
        "b_dense = np.matmul(A,x)\n",
        "print(\"regular = \",b_dense)\n",
        "print(\"sparse - regular = \", b_dense-b_sparse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zce9WusYNAIv",
        "outputId": "3d9a57b0-0419-49a6-ac75-e5358edf373c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sparse =  [7. 3. 1. 5. 1. 5.]\n",
            "regular =  [7 3 1 5 1 5]\n",
            "sparse - regular =  [0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the two methods give the same result. We can also verify this by computing the difference between the two vectors, and we see that it is equal to the zero vector. This result also holds for different input vectors and matrices."
      ],
      "metadata": {
        "id": "OuHMmS_NcVI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QR factorization\n",
        "\n",
        "We now want to verify our result from the QR-factorization. We want to see if R is upper triangular, which is easy to verify by looking at it.\n",
        "\n",
        "We can also check that multiplying Q and R gives us the matrix A back. Another way of checking this is by computing the matrix norm of  $$\n",
        "QR-A.\n",
        "$$\n",
        "If this is 0, the matrices are the same.\n",
        "\n",
        "Another check can be done by looking at $$\n",
        "Q^TQ,\n",
        "$$\n",
        "which should be equal to the identity matrix $I,$ as $Q^T$ is the inverse of $Q.$ Therefor, it follows that the matrix norm if $$\n",
        "Q^TQ - I\n",
        "$$\n",
        "should be 0."
      ],
      "metadata": {
        "id": "gNO3uFXNVNEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[2, -1],[-1,2]])\n",
        "Q, R = QR_factorization(A)\n",
        "\n",
        "print(\"Q = \", Q)\n",
        "print(\"R = \", R)\n",
        "print(\"A = QR = \", np.matmul(Q,R))\n",
        "print()\n",
        "QTQ =  np.matmul(np.transpose(Q),Q)\n",
        "I = np.identity(A.shape[0])\n",
        "print(\"||Q^TQ - I||_F = \", np.linalg.norm(QTQ - I,'fro'))\n",
        "\n",
        "QR = np.matmul(Q,R)\n",
        "print(\"||QR - A||_F = \", np.linalg.norm(QR - A,'fro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBxG4jzeVRa-",
        "outputId": "9ad45c4e-e366-4e3a-830b-bd0be6a90ca7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q =  [[ 0.89442719  0.4472136 ]\n",
            " [-0.4472136   0.89442719]]\n",
            "R =  [[ 2.23606798 -1.78885438]\n",
            " [ 0.          1.34164079]]\n",
            "A = QR =  [[ 2. -1.]\n",
            " [-1.  2.]]\n",
            "\n",
            "||Q^TQ - I||_F =  2.6901577681355055e-16\n",
            "||QR - A||_F =  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the results, we can verify that R is upper triangular, since it has 0 below the diagonal. Also, we can see that $$||QR-A||$$ is equal to 0, so the factorisation gives us the original matrix back. Finally, computing\n",
        "$$\n",
        "||Q^TQ - I||\n",
        "$$\n",
        "gives a answer very close to 0. Considering the presicion with which these computations are done, this confirms that the matrix $Q^T$ is the inverse of $Q,$ and so $Q$ is orthogonal. These results holds for other matrices A as well.\n"
      ],
      "metadata": {
        "id": "ZPtJZoHieIZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solving Ax = b\n"
      ],
      "metadata": {
        "id": "fRWrqzVJvPBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nowm we want to verify that the solver algorithm gives a correct answer. We can check that the norm of $$\n",
        "Ax-b\n",
        "$$\n",
        "is equal to 0, since if the solution is correct, it should hold that $$\n",
        "Ax = b\n",
        "$$\n",
        "We can also construct a solution by solving the system by hand, and comparing that solution, $y,$ to the one found by the algorithm, $x$. We can compute the norm of $$x-y$$ to se if these are equal to each other.    "
      ],
      "metadata": {
        "id": "hO-ddc_Hfhe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1, 3],[0,1]])\n",
        "b = np.array([1,1])\n",
        "\n",
        "x = solver(A,b)\n",
        "y = np.array([-2,1]) # Computed by hand\n",
        "\n",
        "print(\"|Ax-b| = \",np.linalg.norm(np.matmul(A,x)-b))\n",
        "print(\"|x-y| = \",np.linalg.norm(x-y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyqVKwlZvWW_",
        "outputId": "1808e7ee-7642-481a-8c4b-fb99a3523b76"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|Ax-b| =  0.0\n",
            "|x-y| =  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that $Ax-b$ has length zero, so the found solution satisfies the equation. We can also see that it is equal to the solution computed by hand. This holds for other vectors $b$ and matrices $A$ as well."
      ],
      "metadata": {
        "id": "zZWqENMJzA02"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The methods presented in this report had the expected outcomes. For the sparse matrix-vector product, the difference in preformance is not really noticible for the size of matrix used. If a much larger matrix would have been used, a more noticable difference would have beem seen. Also, the built in numpy matrix multiplication already has some optimisations buit in, so a more comparable regular matrix product would have been to implement the matrix multiplication from scratch as well.\n",
        "\n",
        "For the QR-factorization and solving of the $Ax=b$ equation also had the expected outcome. For one of the computations that should have equaled zero, a number the order of magnitude $10^{-16}$ was computed. However, this is to be expected, since computations are not done with infinite precision, and round off errors can result in differences between the expected and actual result."
      ]
    }
  ]
}