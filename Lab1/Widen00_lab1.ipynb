{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT24/blob/Widen00-Lab1/Lab1/Widen00_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **DD2363 Lab 1: Matrix Factorization**\n",
        "**Joel Widén**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "This report is investigating different ways to directly solve matrix equations. A big part of this is factorizing matrices into forms that are more easily calculated or inverted. In this report the methods used are sparse matrix matrix vector multiplication and QR factorization. These methods are used and tested according to stated test cases. All the methods performed as expected which was expected. These methods allows for either faster calculations such as the sparse matrix multiplication or clever ways of dealing with direct matrix equations where the matrix might not be easily invertible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "This is a report about matrix factorization in the course DD2363 Methods in Scientific Computing. The author of this file is Joel Widén, joelwid@kth.se."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "\n",
        "This block is run to set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common\n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "\n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "Numbered algorithms, equations and chapter references used in this report is from the DD2363 course book [Methods in Computational Science by Johan Hoffman](https://epubs.siam.org/doi/book/10.1137/1.9781611976724) if not stated otherwise.\n",
        "\n",
        "This report is investigating different methods of solving the direct matrix equation\n",
        "$Ax = b$ (eq 5.1)\n",
        " by first implementing matrix factorization methods so that $x = A^{-1}b$ can be solved. This is split into several different assignments with the required output, input and test method of the function:\n",
        "\n",
        "**Assignment 1:** Function: sparse matrix-vector product\n",
        "\n",
        "  * *Input:* vector x, sparse (real, quadratic) matrix A: CRS arrays val, col_idx, row_ptr\n",
        "\n",
        "  * *Output:* matrix-vector product $b=Ax$\n",
        "\n",
        "  * *Test:* verify accuracy against dense matrix-vector product.\n",
        "\n",
        "**Assignment 2:** Function: QR factorization\n",
        "\n",
        "  * *Input:* (real, quadratic, invertible) matrix $A$\n",
        "\n",
        "  * *Output:* orthogonal matrix $Q$, upper triangular matrix $R$, such that $A=QR$\n",
        "\n",
        "  * *Test:* $R$ upper triangular, Frobenius norms $|| Q^TQ-I ||_F$ and $|| QR-A ||_F$\n",
        "\n",
        "**Assignment 3:** Function: direct solver $Ax=b$\n",
        "\n",
        "  * *Input:* (real, quadratic) matrix $A$, vector $b$\n",
        "\n",
        "  * *Output:* vector $x=A^-1b$\n",
        "\n",
        "  * *Test:* residual $|| Ax-b ||$, and $|| x-y ||$ where $y$ is a manufactured solution with $b=Ay$\n",
        "\n",
        "To complete assignment 1 algorithm 5.9 from the course book is implemented to calculate the matrix. This algorithm is chosen as it does exactly what is asked from the question as it calculates a sparse matrix vector product.\n",
        "\n",
        "Assignment 2 was completed using algorithm 5.3 which is a modified gram-schmidt agorithm. This is used as the matrix A is factorized as a QR factorization as this specific algorithm does. It outputs Q as a orthogonal matrix and R as an upper triangular matrix.\n",
        "\n",
        "Assignment 3 is solved using the QR-factorization from the previous assignment combined with algorithm 5.2 to solve the resulting problem which involves an upper triangular matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1: Sparse matrix vector product**\n",
        "The sparse matrix algorithm uses the fact that the majority of the entries are zeroes and uses a clever system of pointers and id:s to deconstruct the information of a matrix in a more compact way. This data structure is called CRS or compressed row storage which is described in chapter 5.8. The algorithm can then utilize the fact of the rest of the entries being zeros to perform the matrix vector multiplication by using the CRS data. This is implemented in this assignment."
      ],
      "metadata": {
        "id": "hPoeOoDTOen-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sparse matrix\n",
        "\n",
        "#Sparse matrix vector product based on Algorithm 5.9\n",
        "def sparse_matrix_vector_product(spA, x):\n",
        "  n = max(spA.col_idx)\n",
        "  b = np.zeros(n, dtype=\"int\")\n",
        "  for i in range(0, n):\n",
        "    for j in range(spA.row_ptr[i]-1, spA.row_ptr[i+1]-1):\n",
        "      b[i] = b[i] + spA.val[j]*x[spA.col_idx[j]-1]\n",
        "  return b\n",
        "\n",
        "x_1 = np.array([1, 1, 1, 1, 1, 1])\n",
        "# From example 5.5\n",
        "# Construct a simple sparse matrix class using the CRS data structure\n",
        "class spMatrix:\n",
        "  def __init__(self, val, col_idx, row_ptr):\n",
        "    self.val = val\n",
        "    self.col_idx = col_idx\n",
        "    self.row_ptr = row_ptr\n",
        "\n",
        "# Create a sparse matrix object\n",
        "val = np.array([3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 3])\n",
        "col_idx = np.array([1, 2, 4, 2, 3, 3, 3, 4, 5, 5, 6])\n",
        "row_ptr = np.array([1, 4, 6, 7, 9, 10, 12])\n",
        "spA = spMatrix(val, col_idx, row_ptr)\n",
        "\n",
        "# Creating matrix for comparison dense calculation\n",
        "A_sparse = np.array([[3, 2, 0, 2, 0, 0], [0, 2, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 3, 2, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 2, 3]])\n",
        "\n",
        "#Calling function\n",
        "b_sparse = sparse_matrix_vector_product(spA, x_1)\n",
        "b_dense = A_sparse.dot(x_1)"
      ],
      "metadata": {
        "id": "uKcx6ZE94cOp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2: QR-factorization**\n",
        "\n",
        "QR factorization is a way of factorizing a square matrix $A$ into an orthogonal matrix Q and an upper triangular matrix $R$ such that $A = QR$ (chapter 5.3). This is useful as the inverse of a orthogonal matrix is the same as its transverse according to chapter 2.6. So $Q^T = Q^{-1}$. The upper trianguar matrix R has some useful properties which will be explained in the following assignment.\n"
      ],
      "metadata": {
        "id": "TdGS3xYBRAYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QR factorization. Want to use modified grahm-schmidt algorithm\n",
        "\n",
        "#Algorithm 5.3\n",
        "def modified_gram_schmidt_iteration(A):\n",
        "  n = len(A)\n",
        "  R = np.zeros((n,n))\n",
        "  Q = np.zeros((n,n))\n",
        "  for j in range(0, n):\n",
        "    v = A[:,j]\n",
        "    for i in range(0, j):\n",
        "      R[i][j] = Q[:,i].dot(v)\n",
        "      v = v - R[i][j]*Q[:,i]\n",
        "    R[j][j] = np.linalg.norm(v)\n",
        "    Q[:,j] = v/R[j][j]\n",
        "  return(Q, R)\n",
        "\n",
        "# Vector for QR factorization and direct solving\n",
        "A_inv = np.array([[2, -1], [-1, 2]])\n",
        "#Call function\n",
        "Q, R = modified_gram_schmidt_iteration(A_inv)\n",
        "frob_1 = np.linalg.norm(Q.transpose().dot(Q) - np.identity(len(Q)), \"fro\")\n",
        "frob_2 = np.linalg.norm(Q.dot(R) - A_inv, \"fro\")"
      ],
      "metadata": {
        "id": "O4eM9zqIUB6W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3: Direct solver**\n",
        "To perform this calculation the factorized matrix from assignment 2 is used. This allows a rewrite of $Ax=b$ as\n",
        "\n",
        "$QRx = b ⇒ Rx = Q^Tb$\n",
        "\n",
        "since $QQ^T = I$ according to the definition of orthogonal matrices. This is a new equation which can be solved for $x$ by implementing a backward substitution algorithm since $R$ is an upper triangular matrix."
      ],
      "metadata": {
        "id": "IOfgNLlYRK5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Direct Solver. Previous block must be run before this one.\n",
        "# Direct Solver will be using QR factorization for inverting A using Q and R\n",
        "# from previous assignment. Q is orthogonal and its inverse is equal to Q^T.\n",
        "# Using backward substitution to solve R*x = Q^T*b as Ax = b => QRx = b\n",
        "\n",
        "#Backwards substitution from algorithm 5.2\n",
        "def backward_substitution(U,b):\n",
        "  n = len(b)\n",
        "  x = np.zeros(n)\n",
        "  x[n-1] = b[n-1]/U[n-1,n-1]\n",
        "  for i in range(n-2, -1, -1):\n",
        "    sum = 0\n",
        "    for j in range(i+1, n-2, -1):\n",
        "      sum = sum + U[i][j]*x[j]\n",
        "    x[i] = (b[i] - sum)/U[i][i]\n",
        "  return x\n",
        "\n",
        "#Function calling and vector definitions\n",
        "y = np.array([5, 3])\n",
        "b = A_inv.dot(y)\n",
        "QTb = Q.transpose().dot(b)\n",
        "x = backward_substitution(R, QTb)\n",
        "residual = np.linalg.norm(A_inv.dot(x)-b)\n",
        "error = np.linalg.norm(x-y)"
      ],
      "metadata": {
        "id": "WkEuhuXGNVRu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "The result from the code is presented by running the code block below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results assignment 1\n",
        "print(\"Assignment 1\")\n",
        "if (b_sparse == b_dense).all():\n",
        "  print(\"Question 1 verified with dense matrix\")\n",
        "print(\"Sparse matrix computated b =\", b_sparse)\n",
        "print(\"Dense matrix computated b =\", b_dense)\n",
        "\n",
        "# Results assignment 2\n",
        "print(\"\\n\" + \"Assignment 2\")\n",
        "print(\"Frobenius norm for Q^T*Q - I = \",frob_1)\n",
        "print(\"Frobenius norm for Q*R - A = \",frob_2)\n",
        "print(\"Q = \", Q)\n",
        "print(\"R = \", R)\n",
        "\n",
        "# Results assignment 3\n",
        "print(\"\\n\" + \"Assignment 3\")\n",
        "print(\"Solution x =\", x)\n",
        "print(\"Manufactured solution y =\", y)\n",
        "print(\"Residual: \", residual)\n",
        "print(\"Error: \", error)"
      ],
      "metadata": {
        "id": "e0YmM5A9kLGV",
        "outputId": "5ae63d04-a0c9-4ed4-97d4-e8156647889a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment 1\n",
            "Question 1 verified with dense matrix\n",
            "Sparse matrix computated b = [7 3 1 5 1 5]\n",
            "Dense matrix computated b = [7 3 1 5 1 5]\n",
            "\n",
            "Assignment 2\n",
            "Frobenius norm for Q^T*Q - I =  2.6901577681355055e-16\n",
            "Frobenius norm for Q*R - A =  0.0\n",
            "Q =  [[ 0.89442719  0.4472136 ]\n",
            " [-0.4472136   0.89442719]]\n",
            "R =  [[ 2.23606798 -1.78885438]\n",
            " [ 0.          1.34164079]]\n",
            "\n",
            "Assignment 3\n",
            "Solution x = [5. 3.]\n",
            "Manufactured solution y = [5 3]\n",
            "Residual:  2.6645352591003757e-15\n",
            "Error:  1.9860273225978185e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "From the first assignment the matrix vector product is exactly the same as the dense matrix vector product. This is expected since the operations is identical and only carried out in different ways. The sparse matrix product is vastly more efficient than the dense matrix product though as the complexity of the sparse matrix is $\\mathcal{O}(n)$ compared to the dense matrix multiplication which is of order $\\mathcal{O}(n^2)$ according to chapter 5.8.\n",
        "\n",
        "The second assignment performs the QR factorization of a matrix which allows the original matrix $A$ to ber more easily inverted and therefore solved in matrix equations. Both frobenius norms $|| Q^TQ-I ||_F$ and $|| QR-A ||_F$ are close or equal to zero which means the algorithm outputs $Q$ as an orthogonal matrix and the second norm that $A = QR$. One of the norms is non-zero due to float error. The matrix $R$ is also upper triangular which should be the result of a QR factorization.\n",
        "\n",
        "The last assignment is using the QR factorization to solve the direct equation. The residual and error in this assignment is close to zero and should theoretically be zero but due to float error is not exactly zero. Comparing the outputted vectors reveals that $x$ is the same as the manufactured solution $y$."
      ]
    }
  ]
}