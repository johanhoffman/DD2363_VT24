{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT23/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 1: Matrix Factorization**\n",
        "**Teo Nordström**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        "This file contains the solutions to the three mandatory problems from Lab1 in DD2363, in addition to the solution to one of the optional problems. It is based upon pseudocode and info found in *Methods in Computational Science* by Johan Hoffman (2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
      },
      "source": [
        "\"\"\"This file is based on a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# TEMPLATE INFO:\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se\n",
        "\n",
        "# CODE INFO:\n",
        "# Code written by Teo Nordström 2024, no license."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "These are the neccessary modules for everything in this file to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "All solutions will be partially or entirely based upon the book *Methods in Computational Science* by Johan Hoffman (2021). In the text, it will be referred to as the \"course book\".\n",
        "\n",
        "# Sparse Matrix-Vector Product\n",
        "A sparse matrix is a matrix in which a large percentage of values are zero. Considering the fact that $x \\cdot 0 = 0$ for any $x$, calculating all of these multiplications would be wasteful on processing time and power. To perform this matrix-vector product we do not store the matrix on the standard matrix form, instead opting to store it in a way that contains three lists, these being the values, the column index for each value, and lastly the cutoff points for when we progress to the next row. By doing this, we can save both processing time and memory, as long as the matrix is suitably sparse.\n",
        "\n",
        "# QR Factorization\n",
        "QR Factorization is used to split up a matrix $A$ into component parts $Q$ and $R$ at the form $A = QR$. These matrices in turn have specific desirable properties: $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix (both of which in and of themselves have many desirable properties). There are many methods to perform this factorization but this time Modified Gram-Schmidt QR Factorization will be performed.\n",
        "\n",
        "# Direct Solver\n",
        "As an extension to QR Factorization, a Direct Solver can be used to find the answer to the equation $Ax = b$ (where $A$ is a real quadratic matrix). This is since, while directly calculating the inverse to $A$ may be difficult, calculating the inverse to $Q$ is quite simple due to its properties. The orthogonality of $Q$ means that $Q^{-1}=Q^T$ which is very simple to process. Further, the fact that $R$ is upper triangular means that we can use backwards substitution to calculate the remaining $Rx = b'$ where $b' = Q^{-1}b$.\n",
        "\n",
        "# Least-Squares Problem (bonus)\n",
        "Least-squares is a problem where the solution is the linear solution that gives in total the least difference between the correct solutions and the given solution. By fitting a line to a non-linear problem we can still gather some important information. The form will be a matrix equation $Ax = b$ but where $A$ is of size $m \\times n$ where $m > n$, meaning there will be more equations than unknowns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "# Sparse Matrix-Vector Product\n",
        "To calculate the product of a sparse matrix and a vector we use the pseudocode of Algorithm 5.9 from the course book. It begins by going through each value in the vector $x$ and performing methods using the lists established in the introduction. For each value of $x$ it takes all of the values in the corresponding row of $A$ using the row pointers as a delimitation. Then it does the multiplications one at a time, taking the value from the matrix and multiplying it by the value in the right row of $x$. This is added to the corresponding value in $b$ and is after everything is complete returned."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm_ex = [                                   # Sparse Matrix Example\n",
        "    [3, 2, 2, 2, 1, 1, 3, 2, 1, 2, 3],  # Values\n",
        "    [0, 1, 3, 1, 2, 2, 2, 3, 4, 4, 5],  # Column index\n",
        "    [0, 3, 5, 6, 8, 9, 11]              # Row pointers\n",
        "]\n",
        "\n",
        "def sparse_matrix_vector_product (A, x):\n",
        "    b = []\n",
        "    for i in range(len(A[2]) - 1):\n",
        "        b.append(0)\n",
        "        for j in range(A[2][i], A[2][i+1]):\n",
        "            b[i] += A[0][j]*x[A[1][j]]\n",
        "    return b\n",
        "\n",
        "\n",
        "print(sparse_matrix_vector_product(sm_ex, [1, 1, 1, 1, 1, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_EGvvispQuP",
        "outputId": "223f797e-071a-442e-e6e8-c2f8b47e505a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 3, 1, 5, 1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QR Factorization\n",
        "\n",
        "To factor a matrix $A$ into $Q$ and $R$, Modified Gram-Schmidt Iteration was used. The code is based upon the pseudocode provided by Algorithm 5.3 in the course book. It takes in a square and full-rank matrix $A$, and from this it generates the $Q$ and $R$ factors. By using projection we construct the orthogonal matrix $Q$, and since each step essentially is multiplication with an upper triangular matrix we also get the upper triangular matrix $R$."
      ],
      "metadata": {
        "id": "egnKCwCGvQj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modified_gram_schmidt_iteration(A:np.ndarray):\n",
        "    n = len(A)\n",
        "    Q = np.zeros((n, n))\n",
        "    R = np.zeros((n, n))\n",
        "    for j in range(n):\n",
        "        v = A[:, j]\n",
        "        for i in range(j):\n",
        "            R[i, j] = np.dot(Q[:, i], v)\n",
        "            v = v - R[i, j] * Q[:, i]\n",
        "        R[j, j] = np.linalg.norm(v)\n",
        "        Q[:, j] = v / R[j, j]\n",
        "    return Q, R\n",
        "\n",
        "mod_gsi_A = np.array([[3, 6, 1], [4, 1, 2], [5, 7, -4]])\n",
        "\n",
        "Q, R = modified_gram_schmidt_iteration(mod_gsi_A)\n",
        "print(Q)  # Orthogonal\n",
        "print()\n",
        "print(R)  # Upper Triangular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pAJtD9Kv4bn",
        "outputId": "f40152e4-7a79-437a-b6de-cf56c09ee70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.42426407  0.56273425  0.70945765]\n",
            " [ 0.56568542 -0.77648602  0.27761386]\n",
            " [ 0.70710678  0.28354827 -0.64776568]]\n",
            "\n",
            "[[ 7.07106781  8.06101731 -1.27279221]\n",
            " [ 0.          4.58475735 -2.12443086]\n",
            " [ 0.          0.          3.85574812]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Solver\n",
        "\n",
        "The Direct Solver extends upon the function implemented for QR Factorization. It uses the fact that $Q^{-1}=Q^T$ to simplify the problem $Ax=b → QRx=b → Rx = Q^{-1}b → Rx = Q^Tb$. After this, a simple matrix-vector multiplication can be used to calculate the right hand side of the equation leading to the equation $Rx = b'$. Since $R$ is an upper triangular matrix, backwards substitution can now be used to find $x$ from $R$ and $b'$. The pseudocode for the backwards substitution was provided by Algorithm 5.2 in the course book.  "
      ],
      "metadata": {
        "id": "M6FR-GNhzsPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_substitution(U:np.ndarray, b:np.ndarray):\n",
        "    n = U.shape[1]\n",
        "    x = np.zeros(n)\n",
        "    x[n-1] = b[n-1] / U[n-1, n-1]\n",
        "    for i in range(n-2, -1, -1):\n",
        "        sum = 0\n",
        "        for j in range(i+1, n):\n",
        "            sum += U[i, j] * x[j]\n",
        "        x[i] = (b[i] - sum) / U[i, i]\n",
        "    return x\n",
        "\n",
        "\n",
        "def direct_solver(A:np.ndarray, b:np.ndarray):\n",
        "    Q, R = modified_gram_schmidt_iteration(A)\n",
        "    y = np.transpose(Q).dot(b)\n",
        "    x = backward_substitution(R, y)\n",
        "    return x\n",
        "\n",
        "\n",
        "bs_A = np.array([[2, -1], [-1, 2]])\n",
        "bs_b = np.array([1, 2])\n",
        "\n",
        "print(direct_solver(bs_A, bs_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIULzY2Y2C7O",
        "outputId": "ee1b50ca-406a-492c-98e8-1660a19f7d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.33333333 1.66666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least-Squares Problem (bonus)\n",
        "\n",
        "Least-Squares did not have a direct pseudocode implementation that could be found in the course book, but there was information in Example 2.17 that could be used to develop a functioning Least-Squares solver using the pseudoinverse of $A$. The pseudoinverse is a left inverse version of $A$ since $A^+_LA = I$ where $A^+_L = (A^TA)^{-1}A^T$ is the pseudoinverse. We use this in the form $\\overline{x} = A^+_L b$ which returns the solution to the Least-Squares problem. The inverse of the matrices used in $A^+_L$ may be difficult to calculate on their own, but it is once again possible to QR Factorize this matrix to simplify the process. The function begins by getting the matrix product of $A^TA$ and factoring it into $Q$ and $R$. We now have that $\\overline{x} = (QR)^{-1}A^Tb = R^{-1}Q^{-1}A^Tb$. Since $Q{-1} = Q^T$ we can multiply it with $A^Tb$ and get $\\overline{x} = R^{-1}b''$. We can then move over $R$ to the left hand side $R\\overline{x} = b''$ and use the previously designed backwards substitution function to get the result."
      ],
      "metadata": {
        "id": "yP5CY3YJ2m4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_squares_problem(A:np.ndarray, b:np.ndarray):\n",
        "    Q, R = modified_gram_schmidt_iteration(np.transpose(A) @ A)\n",
        "    ainv = np.transpose(Q) @ (np.transpose(A) @ b)\n",
        "    x = backward_substitution(R, ainv)\n",
        "    return x\n",
        "\n",
        "lsp_A = np.array([[2, -1], [-1, 2], [2, 1]])\n",
        "lsp_b = np.array([1, 2, 1])\n",
        "\n",
        "print(least_squares_problem(lsp_A, lsp_b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FdxcPeL4QL-",
        "outputId": "348d4b81-284f-4c26-b1ee-f32a085c9753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4 0.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "In this section tests will be performed to verify that the solutions are correct\n",
        "\n",
        "# Sparse Matrix-Vector Product\n",
        "To make sure that we are calculating the matrices correctly we will generate a random sparse matrix. It will then be converted to work for the implementation, and then we will compare it to the matrix multiplication implemented in numpy. If difference $= 0 $ in all cases, the function is working.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smvp_test(iters):\n",
        "  for test in range(iters):\n",
        "    m = np.random.randint(5, 10)\n",
        "    n = np.random.randint(5, 10)\n",
        "    sparse_matrix = []\n",
        "    for _ in range(m):\n",
        "        sparse_matrix.append([(np.random.randint(-10, 10) if np.random.random() < 0.25 else 0) for _ in range(n)])\n",
        "\n",
        "    sparse_setup = [[], [], []]\n",
        "    row_pointer_ind = 0\n",
        "    for i, row in enumerate(sparse_matrix):\n",
        "        sparse_setup[2].append(row_pointer_ind)\n",
        "        for j, val in enumerate(row):\n",
        "            if val != 0:\n",
        "                sparse_setup[0].append(val)\n",
        "                sparse_setup[1].append(j)\n",
        "                row_pointer_ind += 1\n",
        "    sparse_setup[2].append(row_pointer_ind)\n",
        "\n",
        "    sparse_matrix = np.array(sparse_matrix)\n",
        "    test_vector = [np.random.randint(-10, 10) for _ in range(n)]\n",
        "\n",
        "    test_product = sparse_matrix_vector_product(sparse_setup, test_vector)\n",
        "    validation_product = sparse_matrix @ np.array(test_vector)\n",
        "\n",
        "    print(f\"Test {test+1}: {np.array(test_product) - validation_product}\" )\n",
        "\n",
        "smvp_test(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5LMH60OBKsP",
        "outputId": "e1ec8d92-c1e3-4d0b-9dc0-20bdd8dae420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: [0 0 0 0 0 0 0 0]\n",
            "Test 2: [0 0 0 0 0 0 0 0 0]\n",
            "Test 3: [0 0 0 0 0 0 0]\n",
            "Test 4: [0 0 0 0 0 0]\n",
            "Test 5: [0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# QR Factorization\n",
        "\n",
        "To make sure that the QR factorization is working correctly we check if $R$ is an upper triangular matrix. This can be done visually, but we iterate to check that all values under the diagonal are 0. We also check the frobenius norms $||Q^TQ-I||$ and $||QR-A||$ which should be colse to 0.\n",
        "\n",
        "We generate matrices until we get an invertible one that can be used for the test."
      ],
      "metadata": {
        "id": "JJc3ibYgBMAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mgsi_test(iters):\n",
        "    for test in range(iters):\n",
        "        n = np.random.randint(4, 8)\n",
        "        matrix = np.random.rand(n, n)\n",
        "        while np.linalg.matrix_rank(matrix) != n:\n",
        "            matrix = np.random.rand(n, n)\n",
        "\n",
        "        Q, R = modified_gram_schmidt_iteration(matrix)\n",
        "\n",
        "        upper_triangular = True\n",
        "        for i, row in enumerate(R):\n",
        "            if i != 0:\n",
        "                for j in range(i):\n",
        "                    if row[j] != 0:\n",
        "                        upper_triangular = False\n",
        "                        break\n",
        "                if not upper_triangular:\n",
        "                    break\n",
        "\n",
        "        frob_qt = np.linalg.norm(Q.T @ Q - np.identity(n))\n",
        "        frob_qr = np.linalg.norm(Q @ R - matrix)\n",
        "\n",
        "        print(f\"Test {test}: Upper Triangular = {upper_triangular}, ||Q^TQ-I|| = {frob_qt}, ||QR -A|| = {frob_qr}\")\n",
        "\n",
        "\n",
        "mgsi_test(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGIKfpRhBMSA",
        "outputId": "85ff94d8-b0b7-4aee-910b-dd38435cd548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: Upper Triangular = True, ||Q^TQ-I|| = 1.612827105125927e-15, ||QR -A|| = 1.38989305853506e-16\n",
            "Test 1: Upper Triangular = True, ||Q^TQ-I|| = 6.761801203493883e-16, ||QR -A|| = 2.34489445204375e-16\n",
            "Test 2: Upper Triangular = True, ||Q^TQ-I|| = 5.916314568483542e-13, ||QR -A|| = 1.0838898772828417e-16\n",
            "Test 3: Upper Triangular = True, ||Q^TQ-I|| = 3.708455557512007e-15, ||QR -A|| = 3.9631554108143483e-16\n",
            "Test 4: Upper Triangular = True, ||Q^TQ-I|| = 2.563575312921917e-15, ||QR -A|| = 3.6424339576766555e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Direct Solver\n",
        "\n",
        "To test the direct solver we just have to see whether it has gotten the right answer. This can be done by checking if the norm of the left-hand side minus the right-hand side is close to zero, aka the residual $||Ax - b||$. We also manufacture our own solution $b = Ay$ to test so the answer the direct solver gives ($x_y$) is the same, which is done using the residual $||x_y - y||$"
      ],
      "metadata": {
        "id": "hWIeJx6OBMhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_test(iters):\n",
        "    for test in range(iters):\n",
        "        n = np.random.randint(2, 10)\n",
        "        matrix = np.random.rand(n, n)\n",
        "        while np.linalg.matrix_rank(matrix) != n:\n",
        "            matrix = np.random.rand(n, n)\n",
        "        vector = np.random.rand(n)\n",
        "\n",
        "        x = direct_solver(matrix, vector)\n",
        "        res_axb = np.linalg.norm(matrix @ x - vector)\n",
        "\n",
        "        y = np.random.rand(n)\n",
        "        x_y = direct_solver(matrix, matrix @ y)\n",
        "        res_xy = np.linalg.norm(x_y - y)\n",
        "        print(f\"Test {test}: ||Ax-b|| = {res_axb}, ||x_y-y|| = {res_xy}\")\n",
        "\n",
        "\n",
        "ds_test(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hge11-v_BMww",
        "outputId": "64f2abe0-270c-401f-aec3-ff02f99ab302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: ||Ax-b|| = 8.005932084973442e-16, ||x_y-y|| = 9.226046854752787e-15\n",
            "Test 1: ||Ax-b|| = 1.7010341408823585e-15, ||x_y-y|| = 6.656664898175294e-14\n",
            "Test 2: ||Ax-b|| = 1.2412670766236366e-16, ||x_y-y|| = 7.850462293418876e-16\n",
            "Test 3: ||Ax-b|| = 2.220446049250313e-16, ||x_y-y|| = 1.2456232622454378e-16\n",
            "Test 4: ||Ax-b|| = 3.780290039183201e-15, ||x_y-y|| = 1.9188705342173193e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Least-Squares Problem (bonus)\n",
        "\n",
        "To see if our Least-Squares implementation is working we will use the residual $||A\\overline{x}-b||$. The residual on its own will not tell us that much, however, considering that the result will not be exact. We will therefore have to refer to a known good solution of the problem: the function in numpy that solves the least-squares problem. By comparing the residual we get to the residual the function gets, we can see if both agree, in which case it is highly probable to be the correct solution."
      ],
      "metadata": {
        "id": "FSFS1_8FBQjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lsp_test(iters):\n",
        "    for test in range(iters):\n",
        "        n = np.random.randint(5, 10)\n",
        "        m = n + np.random.randint(5, 10)\n",
        "        matrix = np.random.rand(m, n)\n",
        "        vector = np.random.rand(m)\n",
        "\n",
        "        x = least_squares_problem(matrix, vector)\n",
        "        res_axb = np.linalg.norm(matrix @ x - vector)\n",
        "\n",
        "        x_np = np.linalg.lstsq(matrix, vector, rcond=None)\n",
        "        res_axb_np = np.linalg.norm(matrix @ x_np[0] - vector)\n",
        "\n",
        "        print(f\"Test {test}: Own = {res_axb}, numpy = {res_axb_np}, diff = {res_axb - res_axb_np}\")\n",
        "\n",
        "\n",
        "lsp_test(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl64D32nBQzJ",
        "outputId": "18361984-a944-44eb-a582-c46143dd8204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 0: Own = 0.9660874836445156, numpy = 0.9660874836445157, diff = -1.1102230246251565e-16\n",
            "Test 1: Own = 1.0409659470914951, numpy = 1.0409659470914951, diff = 0.0\n",
            "Test 2: Own = 0.750878647739558, numpy = 0.750878647739558, diff = 0.0\n",
            "Test 3: Own = 0.37202665725379663, numpy = 0.3720266572537966, diff = 5.551115123125783e-17\n",
            "Test 4: Own = 0.9619465162838096, numpy = 0.9619465162838097, diff = -1.1102230246251565e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The results of the methods were about as expected as possible. They are very close to the example algorithms provided in the book and therefore I have no reason to believe that they should not work. While only a few of the test cases got to a value of exactly zero (which would be the perfect proof) all of them came close enough to zero to say that they are well within tolerances."
      ]
    }
  ]
}