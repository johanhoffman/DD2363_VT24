{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 6: Optimization and learning**\n",
        "**Matteus Berg**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "In this lab we implement the gradient descent method for the unconstrained minimization problem. The method is verified for its accuracy and convergence by testing. The tests show that the implementation correctly approximates critical points when the tolerance is below a certain threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01b00aa5-12e1-49e0-891e-f1a601394eab"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common\n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "\n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "This lab report aims to implement the gradient descent method to solve the unconstrained minimization problem. The minimization problem consists of finding a critical minimum point for a function $f$, known as an objective function. If the problem is unconstrained, it means that the search space is $R^n$ and not a subamount of $R^n$. Gradient descent is an iterative method. The iteration equation is described on page 327 of the book as\n",
        "\n",
        "$x^{k+1} = x^{(k)} - \\alpha^{(k)}\\nabla f(x^{(k)})$\n",
        "\n",
        "This report implements the gradient descent method pseudocode that can be found on page 327 of the book. The method is implemented to support a function $f : D \\rightarrow R$. Where $D \\subseteq R^N$. That is, the function may have a vector as input, but its output is always scalar. Gradient descent should stop iterating at a point $x^*$ that produces a gradient whose magnitude is smaller than the tolerance. That is, $|| \\nabla f(x^*) || < TOL$.\n",
        "\n",
        "$\\alpha$ should be defined so that the right step length is taken. Taking too small steps will lead to a performance decrease. However, taking too large steps will lead to the method overshooting the critical point. Calculating the correct step length is a difficult problem, and in general the calculations need to be adjusted depending on what the objective function looks like. For this implementation, $\\alpha$ is calculated using an iterative line search method. The line search method iterates until it gets an interval smaller than the tolerance. Number of iterations for line search decide the size of $\\alpha$ for that gradient descent step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "\n",
        "### Gradient calculation\n",
        " The gradient $\\nabla f(x)$ for each step is calculated by the compute_gradient function. We have a point $p_0$ that we want to calculate the gradient for. First we determine a step size $step$ for the gradient, this is equal to $TOL * 0.1$. For a point $x_k$ we have $n$ dimensions. For each dimension $d$, we compute three points $p_1$, $p_2$, $p_3$: $p_1(x_1, ..., x_d - step, ..., x_n)$, $p_2(x_1, ..., x_d, ..., x_n)$, $p_3(x_1, ..., x_d + step, ..., x_n)$. Observe that $p_2$ will always be equal to $p_0$ Using these three points we calculate the gradient for $d$ in the point $p0$ using the central difference method. The separate gradients are then merged together to get the complete $n$-dimensional gradient array $\\nabla f(x)$.\n",
        "\n",
        "\n",
        "### $\\alpha$-calculation\n",
        "$\\alpha$ is calculated in the line_search method. We want to search for the critical point $p_c$ from the starting point $p_0$ In each iteration, three points are present:\n",
        "1. $s_1$ the lower bound of the interval\n",
        "2. $s_2$ the upper bound of the interval\n",
        "3. $s_3$ the midpoint between $s_1$ and $s_2$\n",
        "\n",
        "\"the interval\" refers to the search interval for the method. It is an $n$-dimensional array, each element representing the interval length for one of the variables that $f$ takes as input. Before we begin the first iteration, we need to get a starting search interval which captures the critical point. This is done by setting each interval element to its respective gradient value in $p_0$: $interval = \\nabla f(p_0)$. Now the iterations can begin. For each iteration:\n",
        "1. For every dimension $i$:\n",
        "    1. if gradient is negative\n",
        "        1. set $s_2[i]$ to $s_1[i] + interval[i]$\n",
        "    2. if gradient is positive or zero\n",
        "        1. set $s_2[i]$ to $s_1[i]$\n",
        "        2. set $s_1[i]$ to $s_1[i] - interval[i]$\n",
        "2. calculate $s_3$\n",
        "3. Out of the three points, pick the two points with lowest function values and set them to $s_1$ and $s_2$:\n",
        "```python\n",
        "if f(s1) > f(s3):\n",
        "      s1 = s3\n",
        "    if (not np.array_equal(s1,s3)) and (f(s2) > f(s3)):\n",
        "      s2 = s3\n",
        "```\n",
        "\n",
        "4. update gradient to $\\nabla f(s_1)$\n",
        "5. update interval to $abs(s2 - s1)$\n",
        "6. increment iterations variable\n",
        "\n",
        "The algorithm stops iterating when the norm of the gradient is lower than the tolerance. With this algorithm we are guaranteed to half the search space each iteration. So the algorithm has complexity $O(log_2(n))$. The higher the iteration value, the higher the $\\alpha$ value returned. So in short, the further away $p_0$ is from the critical point, the higher the $\\alpha$ value will be."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tolerance\n",
        "TOL = 0.001\n",
        "\n",
        "# implemented pseudocode from page 327 of the book\n",
        "def gradient_descent(f, x0):\n",
        "  x = x0\n",
        "  Df = compute_gradient(f, x)\n",
        "  while np.linalg.norm(Df) > TOL:\n",
        "    Df = compute_gradient(f, x)\n",
        "    alpha = line_search(f, x)\n",
        "    #print(\"x = x - alpha*Df :\", x - alpha*Df, \"=\", x, \"-\", alpha, \"*\", Df)\n",
        "    x = x - alpha*Df\n",
        "  return x\n",
        "\n",
        "# line interval search\n",
        "# this algorithm only converges if it manages to\n",
        "# capture a critical point in its starting interval\n",
        "def line_search(f, x):\n",
        "  n = x.size\n",
        "  interval = np.zeros(n)\n",
        "  numIterations = 1\n",
        "  s1 = np.zeros(n)\n",
        "  s2 = np.zeros(n)\n",
        "  s3 = np.zeros(n)\n",
        "  grad = compute_gradient(f, x)\n",
        "  interval = grad\n",
        "\n",
        "  for i in range(n):\n",
        "    if(grad[i] < 0):\n",
        "      s1[i] = x[i]\n",
        "      s2[i] = x[i] + interval[i]\n",
        "    else:\n",
        "      s2[i] = x[i]\n",
        "      s1[i] = x[i] - interval[i]\n",
        "\n",
        "  while np.linalg.norm(s2 - s1) > TOL:\n",
        "    numIterations = numIterations + 1\n",
        "    grad = compute_gradient(f, s1)\n",
        "    interval = abs(s2 - s1)\n",
        "    for i in range(n):\n",
        "      if(grad[i] < 0):\n",
        "        s1[i] = s1[i]\n",
        "        s2[i] = s1[i] + interval[i]\n",
        "      else:\n",
        "        s2[i] = s1[i]\n",
        "        s1[i] = s1[i] - interval[i]\n",
        "    s3 = (s1+s2)/2\n",
        "\n",
        "    if f(s1) > f(s3):\n",
        "      s1 = s3\n",
        "    if (not np.array_equal(s1,s3)) and (f(s2) > f(s3)):\n",
        "      s2 = s3\n",
        "\n",
        "  # even if no iterations of while loop is made, numIterations will be 1\n",
        "  # which means we won't return zero\n",
        "  return numIterations*TOL*10\n",
        "\n",
        "\n",
        "def compute_gradient(f, x0):\n",
        "  alpha = TOL*0.1\n",
        "  n = x0.size\n",
        "  # n trios of points ; one out of three points\n",
        "  f_values = np.zeros((n, 3))\n",
        "  # (n trios of points ; one out of three points ; one coordinate in one point)\n",
        "  x = np.zeros((n, 3, n))\n",
        "  # n gradient values\n",
        "  grad = np.zeros(n)\n",
        "  diffx = np.array([x0-alpha, x0, x0+alpha])\n",
        "  # generate x-values\n",
        "  for i in range(n):\n",
        "    for j in range(3):\n",
        "      for k in range(n):\n",
        "        if i == k:\n",
        "          x[i][j][k] = diffx[j][i]\n",
        "        else:\n",
        "          x[i][j][k] = x0[k]\n",
        "\n",
        "  #print(x)\n",
        "  # generate function values\n",
        "  for i in range(n):\n",
        "    for j in range(3):\n",
        "      f_values[i][j] = f(x[i][j])\n",
        "\n",
        "  # calculate gradient with central difference\n",
        "  for i in range(n):\n",
        "    # creates approximated gradient for three points. We are only interessted\n",
        "    # in the middle point\n",
        "    gradients = np.gradient(f_values[i], alpha)\n",
        "    # give middle point (xi) as gradient value to grad\n",
        "    grad[i] = gradients[1]\n",
        "\n",
        "  # return gradient\n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "oqTmmVPxzAGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([15])\n",
        "f1 = lambda x : x*x\n",
        "x2 = np.array([0, 0])\n",
        "f2 = lambda x : x[0]*x[0] + x[1]*x[1] + 2*x[0] + 4*x[1] + 5\n",
        "\n",
        "TOL = 0.01\n",
        "print(\"Testing gradient descent for f in R1. Tolerance=0.01\")\n",
        "print(\"x0 = 15 ; f(x) = x^2\")\n",
        "print(\"Expected result:\", 0)\n",
        "print(\"Actual result:\", gradient_descent(f1, x1))\n",
        "TOL = 0.001\n",
        "print(\" \")\n",
        "print(\"Testing gradient descent for f in R1. Tolerance=0.001\")\n",
        "print(\"x0 = 15 ; f(x) = x^2\")\n",
        "print(\"Expected result:\", 0)\n",
        "print(\"Actual result:\", gradient_descent(f1, x1))\n",
        "TOL = 0.0001\n",
        "print(\" \")\n",
        "print(\"Testing gradient descent for f in R1. Tolerance=0.0001\")\n",
        "print(\"x0 = 15 ; f(x) = x^2\")\n",
        "print(\"Expected result:\", 0)\n",
        "print(\"Actual result:\", gradient_descent(f1, x1))\n",
        "TOL = 0.001\n",
        "print(\" \")\n",
        "print(\"Actual result:\", gradient_descent(f1, x1))\n",
        "print(\"Testing gradient descent for f in R2. Tolerance=0.001\")\n",
        "print(\"x0 = [0, 0] ; f(x,y) = x^2 + y^2 + 2x + 4y + 5\")\n",
        "print(\"Expected result: [\", -1, \", \", -2, \"]\")\n",
        "print(\"Actual result:\", gradient_descent(f2, x2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LGmZVtkcYVG",
        "outputId": "b9bafdb0-e8d1-4e99-9820-b57d36bd94c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing gradient descent for f in R1. Tolerance=0.01\n",
            "x0 = 15 ; f(x) = x^2\n",
            "Expected result: 0\n",
            "Actual result: [-2.59929314e+14]\n",
            " \n",
            "Testing gradient descent for f in R1. Tolerance=0.001\n",
            "x0 = 15 ; f(x) = x^2\n",
            "Expected result: 0\n",
            "Actual result: [0.00048515]\n",
            " \n",
            "Testing gradient descent for f in R1. Tolerance=0.0001\n",
            "x0 = 15 ; f(x) = x^2\n",
            "Expected result: 0\n",
            "Actual result: [4.98484388e-05]\n",
            " \n",
            "Actual result: [0.00048515]\n",
            "Testing gradient descent for f in R2. Tolerance=0.001\n",
            "x0 = [0, 0] ; f(x,y) = x^2 + y^2 + 2x + 4y + 5\n",
            "Expected result: [ -1 ,  -2 ]\n",
            "Actual result: [-0.99978609 -1.99957217]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "Running the results code, we see that the algorithm outputs correct estimations with the chosen tolerance level. It does this both for a function domain of one and several variables. When the tolerance is raised to $10^{-2}$, the method diverges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The algorithm behaved as expected and converges for both $D = R^1$ and $D = R^2$ when $TOL \\leq 10^{-3}$. However, when the tolerance is raised to $10^{-2}$ the method diverges. This is most probably due to the line_search method calculating too high $\\alpha$ values when $TOL$ is that high. The line_search method would in that case have to be modified and not depend as much on the value of $TOL$ when calculating $\\alpha$."
      ]
    }
  ]
}