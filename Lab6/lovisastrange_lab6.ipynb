{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 6: Optimization**\n",
        "**Lovisa Strange**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "In this lab report, a method for finding an extreme point of a functions is presented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e4193bc-19de-43ea-f9b2-41ae70653f9b"
      },
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2024 Lovisa Strange (lstrange@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.optimize import fsolve\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common\n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "\n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "An important problem in mathematics is finding the maximum (or minimum) of a function. This can be done analytically, for example by looking at the derivative of a function.\n",
        "\n",
        "There are also different types of minimixation problems. (Methods in Computational Science, p.325, Johan Hoffman) For example, we can minimizie a function without any constraint. Another version is constrained minimization problems, where we have some condition on the solution. It could for example be $$\n",
        "\\begin{cases}\n",
        "min \\;\\; f(x_1,x_2)\\\\\n",
        "x_1+x_2 = 1\n",
        "\\end{cases}\n",
        "$$\n",
        "We also have global extreme points, which is the minimum or maximum for a function on the whole domain. A local extreme point on the other hand is only guaranteed to be a minimal or maximal point locally aroundthe point. It is also worth to note that the  minimum of a function $f(x)$ is the same as the maximum of the function $-f(x),$ so it is enough to discuss one of these two cases. Any method for finding a minimum point applied on $-f(x)$ is then a method of finding a maximum point.\n",
        "\n",
        "However, it is also useful to be able to minimize a function numerically, and one example of such a function is the gradient descent method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "##Gradient descent method in $R^n$\n",
        "The gradient descent method (Methods in Computational Science, p.327, Johan Hoffman) is based on searching for a minimum of the function in the direction opposite to the gradient of the function in that point. We know that the gradient is orthogonal to the level curves $$\n",
        "L_c(f) = \\{x\\in D: f(x) = c\\}.\n",
        "$$\n",
        "The negative direction of the gradient can then be used to move closer to a minimum of the function.\n",
        "\n",
        "Using Taylors theorem, we get $$\n",
        "f(x+ \\Delta x) - f(x) = ∇ f(x)^T Δx + \\mathcal{O}(||Δx||^2),\n",
        "$$\n",
        "and we want to minimize $$∇ f(x)^T Δx.$$ This is the case if $Δx$ is in the direction of $-\\nabla f.$ We get the iterative method $$\n",
        "x^{(k+1)} = x^{(k)} - α^{(k)}∇f(x^{(k)}).\n",
        "$$\n",
        "\n",
        "In each step, the step length $α^{(k)}$ is computed. Thos can be done by searching for the minimum of the one-dimentional function $$\n",
        "f(s) = f(x^{(k)}-s∇ f(x^{(k)})).\n",
        "$$\n",
        "We can do this using a golden ratio line search method. This is done by picking an interval, and computing two interior points $s_i$, keeping the ratio between the length of the sub-intervals the same. Depending on which point has the samllest function value, we pick one of the points as a new end point for the interval. This is done iteratively until the interval is small enough. We then get a value for the step length. This is done in the algorithm below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Golden ratio line search\n",
        "\n",
        "# Input: Function f, vector x\n",
        "# Output: s so that f(x-s*grad(f)) is minimized\n",
        "\n",
        "def line_search(f,x):\n",
        "  tolerance = 10**(-6)\n",
        "  phi = (np.sqrt(5)-1)/2\n",
        "\n",
        "  max_iterations = 100\n",
        "  iterations = 0\n",
        "  grad_f = np.array(gradient(x))\n",
        "\n",
        "  s_a  = 0\n",
        "  s_b = 2\n",
        "\n",
        "  s3 = s_a + (1-phi)*(s_b-s_a)\n",
        "  s4 = s_a + phi*(s_b-s_a)\n",
        "\n",
        "  f_3 = f(x - s3*grad_f)\n",
        "  f_4 = f(x - s4*grad_f)\n",
        "\n",
        "  while abs(s_a-s_b) > tolerance and max_iterations > iterations:\n",
        "    iterations +=1\n",
        "\n",
        "    if f_3 < f_4:\n",
        "      s_b = s4\n",
        "      s4 = s3\n",
        "      s3 = s_a + (1-phi)*(s_b-s_a)\n",
        "      f_4 = f_3\n",
        "      f_3 = f(x - s3*grad_f)\n",
        "    else:\n",
        "      s_a = s3\n",
        "      s3 = s4\n",
        "      s4 = s_a + phi*(s_b-s_a)\n",
        "      f_3 = f_4\n",
        "      f_4 = f(x - s4*grad_f)\n",
        "\n",
        "  return (s_a+s_b)/2\n",
        "\n"
      ],
      "metadata": {
        "id": "mus1rnzVmkVq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we have the algorithm for the gardient descent method."
      ],
      "metadata": {
        "id": "kM7TA8udk0q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Jacobi iteration, based on Algorithm 15.1 (gradient_descent_method), p.327, Methods in Computational Science\n",
        "\n",
        "# Input: Function f, starting guess x0\n",
        "# Output: solution x\n",
        "\n",
        "def gradient_descent(f,x0):\n",
        "  tolerance = 10**(-10)\n",
        "  x = np.array(x0)\n",
        "  Df = np.array(gradient(x))\n",
        "  i = 0\n",
        "\n",
        "  while np.linalg.norm(Df) > tolerance:\n",
        "\n",
        "    if i % 1000 ==0: # For printing convergence\n",
        "      print(\"Iteration: \",i,\" x = \", x)\n",
        "\n",
        "    Df = np.array(gradient(x))\n",
        "    alpha = line_search(f,x)\n",
        "\n",
        "    x = x - alpha*Df\n",
        "    i+=1\n",
        "\n",
        "  return x\n",
        "\n"
      ],
      "metadata": {
        "id": "gCWTJPEKC3Vp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**\n",
        "In this section, the results produced by the algorithm in the previous section are presented"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient descent method in $R^n$"
      ],
      "metadata": {
        "id": "pNraLTPrNIwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define a function $$\n",
        "f(x_1,x_2) = x_1^2+x_2^2 +x_1x_2 + 3x_1+2x_2 + 20.\n",
        "$$\n",
        "It has a gradient of $$\n",
        "∇ f (x_1,x_2) = [2x_1+x_2+3, 2x_2+x_1+2]^T.\n",
        "$$\n",
        "We also know that it has a minimum in $$\n",
        "(x_1,x_2) = (-4/3, -1/3).\n",
        "$$"
      ],
      "metadata": {
        "id": "bz8cFdATlC_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 20 + x[0]**2 + x[1]**2 +x[0]*x[1] + 3*x[0] +2*x[1]"
      ],
      "metadata": {
        "id": "6YMrXi3qgHbc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x):\n",
        "  return [2*x[0] + x[1]+3,2*x[1]+x[0]+2]"
      ],
      "metadata": {
        "id": "Xo4skHCNVgOe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the convergence of the solution by printing the solution vector at some iterations during the calculation.\n",
        "\n",
        "Another way of checking the result is by comparing the analytical solution to the computed one, using an initial guess $$\n",
        "x_0 = [-5,-1]\n",
        "$$"
      ],
      "metadata": {
        "id": "X6IGaj4UItCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"convergence of x-values:\")\n",
        "num_sol = gradient_descent(f,[-5,-1])\n",
        "exact_sol = [-4/3,-1/3]\n",
        "print(\" \")\n",
        "print(\"exact solution - nummerical solution = \", exact_sol-num_sol)\n",
        "\n"
      ],
      "metadata": {
        "id": "LxX5dbDOP89h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4509df-c4ad-41d0-a24e-2a06c83d7f84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence of x-values:\n",
            "Iteration:  0  x =  [-5 -1]\n",
            "Iteration:  1000  x =  [-1.33333328 -0.33333328]\n",
            "Iteration:  2000  x =  [-1.33333333 -0.33333333]\n",
            "Iteration:  3000  x =  [-1.33333333 -0.33333333]\n",
            "Iteration:  4000  x =  [-1.3333333 -0.3333333]\n",
            "Iteration:  5000  x =  [-1.33333331 -0.33333331]\n",
            "Iteration:  6000  x =  [-1.33333334 -0.33333334]\n",
            "Iteration:  7000  x =  [-1.33333331 -0.33333331]\n",
            "Iteration:  8000  x =  [-1.33333336 -0.33333336]\n",
            " \n",
            "exact solution - nummerical solution =  [8.23949797e-11 8.23945356e-11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the solution converges to the exact solution. We can also see that the exact and nummerical solutions are close to each other, as they should be. This also holds for other functions f."
      ],
      "metadata": {
        "id": "_FPV0v8jmYSw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The results were mostly expected. The solution converged to the expected value, after trying some different starting points which converged at different rates. Different functions also affected the convergence rate of the function. For example, it was harder to find a starting point that made the method converge for some functions that were too flat around the minimum. The flatness affects the gradient of the function, which makes the solution converge very slowly for some cases."
      ]
    }
  ]
}